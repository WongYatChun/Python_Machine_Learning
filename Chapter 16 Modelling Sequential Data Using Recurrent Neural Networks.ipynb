{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16 Modelling Sequential Data Using Recurrent Neural Network\n",
    "\n",
    "### In this chapter, we will cover\n",
    "- Introducing sequential data\n",
    "- RNNs for modelling sequences\n",
    "- Long Short-Term Memory (LSTM)\n",
    "- Truncated Backpropagation Through Time (T-BPTT)\n",
    "- Implementing a multilayer RNN for sequential modeling in TensorFlow\n",
    "- Project 1: RNN sentiment analysis of the IMDb movie review dataset\n",
    "- Project 2: RNN character-level languge modeling with LSTM cells, using text data from Shakspeare's Hamlet\n",
    "- Using gradient clipping to avoid  exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing sequential data\n",
    "- Sequential data: elements in a sequence appear in a certain order\n",
    "    - are not independent of each other\n",
    "    - order matters\n",
    "    - Unlike typical machine learning algorithms for supervised learning which assums the input data is IID\n",
    "        - MLPs and CNNs are not capable of handling the order of input samples\n",
    "        - they do not have memory of the past seen samples\n",
    "            - weights are updated independent of the order in which the sample is processed\n",
    "    - RNN:\n",
    "        - designed for modeling sequences\n",
    "        - capable of remembering past information and processing new events accordingly\n",
    "        - Application: e.g. language translation, image captioning, text generation\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories of sequence modeling\n",
    "- Many-to-one e.g. Sentiment Analysis\n",
    "     - input: a squence e.g.text\n",
    "     - output: a fixed-size vector, not a sequence e.g.class label\n",
    "     \n",
    "- One-to-many e.g. Image Captioning\n",
    "    - input: a fixed-sized vector e.g. image\n",
    "    - output: a sequence e.g. An English phrase\n",
    "\n",
    "- Many-to-many e.g.Language Translation (Delayed) Video classification (Synchronized)\n",
    "    - input: a sequence e.g. An English phrase\n",
    "    - output: a sequence e.g. An German phrase\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for modeling sequences\n",
    "\n",
    "- layer 1:\n",
    "    - the hidden layer: $h_1^(t)$\n",
    "    - input from data point: $x_t$\n",
    "    - input from the hidden values in the same layer, but the previous time step $h_1^(t-1)$\n",
    "    \n",
    "- layer 2:\n",
    "    - the hidden layer: $h_2^(t)$\n",
    "    - input from $h_1^(t)$\n",
    "    - input from its own hidden values from prevous time step $h_2^(t-1)$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing activations in an RNN\n",
    "\n",
    "- $W_{xh}$: The weight matrix between the input $x^t$ and the hidden layer h\n",
    "- $W_{hh}$: The weight matrix associated with the recurrent edge\n",
    "- $W_{hy}$: The weight matrix between the hidden layer and output layer\n",
    "- $W_h = [W_{xh} + W_{hh}]$ combined weight matrices\n",
    "- $b_h$: Bias vector for the hidden units\n",
    "- $z_h$: Net input\n",
    "- $\\phi_h(.)$: the activation function of the hidden layer\n",
    "\n",
    "Formula for computing hidden units:\n",
    "$$ h^t = \\phi(W_h[x^t,h^(t-1)]+b_h) $$\n",
    "\n",
    "Activation of output units:\n",
    "$$ y^t = \\phi(W_{hy}^t+b_y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenges of learning long-range interactions\n",
    "\n",
    "- Vanishing or exploding gradient problem\n",
    "    - Vanishing: $|w|<1$\n",
    "    - exploding: $|w|>1$\n",
    "    - Ideal: $|w|=1$\n",
    "\n",
    "- Solutions:\n",
    "    - Truncated backpropagation through time (TBPTT)\n",
    "        - clip the gradients above a given threshold\n",
    "        - limit the number of steps that gradient can effectively flow back and properly update the weights\n",
    "    - Long short-term memory (LSTM)\n",
    "        - Most popular\n",
    "        - introduce a memory cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM units\n",
    "\n",
    "- Building block: a Memory Cell\n",
    "- Cell state at the current time step $C^t$: value associated with the current edge\n",
    "- 3 different types of gates:\n",
    "    - Forget gate ($f_t$): allow the memory cell to reset the cell state without growing indefinitely\n",
    "        - decide which information is allowed to go through and which information to suppress\n",
    "        - $f_t = \\sigma(W_{xf}x^t+W_{hf}h^{t-1}+b_f)$\n",
    "    - Input gate ($i_t$) and input node($g_t$): update the cell state\n",
    "        - $i_t = \\sigma(W_{xi}x^t+W_{hi}h^{t-1}+b_i)$\n",
    "        - $g_t = \\sigma(W_{xg}x^t+W_{hg}h^{t-1}+b_g)$\n",
    "    - The cell state at time $t$:\n",
    "        - $C^t = (C^{t-1} \\bigodot f_t) \\bigoplus (i_t \\bigodot g_t)$\n",
    "            - $\\bigodot$: element wise product\n",
    "            - $\\bigoplus$: element wise summation\n",
    "    - Output gate ($o_t$): decide how to update the values of hidden units:\n",
    "        - $o_t = \\sigma(W_{xo}x^t+W_{ho}h^{t-1}+b_o)$\n",
    "    - Hidden units at the current time step:\n",
    "        - $h^t = o_t \\bigodot tanh(C^t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a multilayer RNN for sequence modeling in TensorFlow\n",
    "\n",
    "### Project 1: Performing sentiment analysis of IMDb movie reviews using multilayer RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  This tearful movie about a sister and her batt...          1\n",
      "1  It's too kind to call this a \"fictionalized\" a...          0\n",
      "2  Truly bad and easily the worst episode I have ...          0\n"
     ]
    }
   ],
   "source": [
    "## Import the necessary modules and read the data into a DataFrame pandas\n",
    "\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding = 'utf-8')\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:05:26\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing the data:\n",
    "## Separate words and\n",
    "## count each word's occurrence\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title = 'Counting words occurences')\n",
    "\n",
    "for i, review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' ' +c+' '\\\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "## Create a mapping\n",
    "## Map each unique word to an integer\n",
    "\n",
    "word_counts = sorted(counts, key = counts.get, reverse = True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title = 'Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define fixed-length sequences:\n",
    "## Use the last 200 elements of each sequence\n",
    "## if sequence length < 200: left-pad with zeros\n",
    "\n",
    "sequence_length = 200 ## sequence length (or T in our formulas)\n",
    "\n",
    "sequences = np.zeros((len(mapped_reviews),sequence_length),dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i,-len(row):] = review_arr[-sequence_length:]\n",
    "\n",
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "## Function to generate minibatches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[: n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0,len(x),batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else: \n",
    "            yield x[ii:ii+batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "### Build the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size = 256, num_layers = 1, batch_size = 64,\n",
    "                 learning_rate = 0.0001, embed_size = 200):\n",
    "        # creating the embedding layer\n",
    "        self.n_words = n_words # = the number of unique words (plus 1 since we use zero to fill sequences who size is less than 200)\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.seq_len = seq_len # \n",
    "        self.lstm_size = lstm_size ## number of hidden units in each RNN layer\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                              shape = (self.batch_size, self.seq_len),\n",
    "                              name = 'tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                              shape = (self.batch_size),\n",
    "                              name = 'tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                     name = 'tf_keepprob')\n",
    "        \n",
    "        ## Create the embedding layer\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        (self.n_words, self.embed_size),\n",
    "                        minval = -1, maxval = 1),\n",
    "                    name = 'embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "                    embedding, tf_x, \n",
    "                    name = 'embeded_x')\n",
    "        ## Step 1: Define LSTM cell and stack them together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell( #3. Make a list of such cells \n",
    "                [tf.contrib.rnn.DropoutWrapper( #2. Apply the dropout to the RNN cells\n",
    "                    tf.contrib.rnn.BasicLSTMCell(self.lstm_size),## 1. Create the RNN cells\n",
    "                    output_keep_prob = tf_keepprob) #2. Apply the dropout to the RNN cells\n",
    "                 for i in range(self.num_layers)])#3. According to the desired number of RNN layers\n",
    "        \n",
    "        ## Step 2: Define the initial state:\n",
    "        self.initial_state = cells.zero_state(\n",
    "                    self.batch_size, tf.float32)\n",
    "        print('   << initial state >> ', self.initial_state)\n",
    "        \n",
    "        ## Step 3: Creating the RNN using the RNN cells and their states\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                            cells, embed_x,\n",
    "                            initial_state = self.initial_state)\n",
    "        \n",
    "        ## Note: lstm_outputs shape:\n",
    "        ##    [batch_size, max_time, cells.output_size]\n",
    "        print('   << lstm_output >> ', lstm_outputs)\n",
    "        print('   << final state >> ', self.final_state)\n",
    "        \n",
    "        ## Apply a FC layer after on top of RNN output\n",
    "        logits = tf.layers.dense(\n",
    "                    inputs = lstm_outputs[:,-1],\n",
    "                    units = 1, activation = None,\n",
    "                    name = 'logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name = 'logits_squeezed')\n",
    "        print('   << logits >> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name = 'probabilities')\n",
    "        \n",
    "        predictions = {\n",
    "            'probabilities' : y_proba,\n",
    "            'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
    "                               name = 'labels')\n",
    "        }\n",
    "        print('\\n   << predictions >> ', predictions)\n",
    "        \n",
    "        ## Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels = tf_y, logits = logits),\n",
    "                             name = 'cost')\n",
    "        \n",
    "        ## Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name = 'train_op')\n",
    "    \n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range (num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                        X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state : state}\n",
    "                    loss, _, state = sess.run(\n",
    "                                ['cost:0', 'train_op',\n",
    "                                 self.final_state],\n",
    "                                 feed_dict = feed)\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d\"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                              epoch + 1, num_epochs,\n",
    "                              iteration, loss))\n",
    "                    iteration += 1\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                                    \"model/sentiment-%d.ckpt\" % epoch)\n",
    "                    \n",
    "    def predict(self, X_data, return_proba = False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(create_batch_generator(\n",
    "                                X_data, None, batch_size = self.batch_size),1):\n",
    "                feed = {\"tf_x:0\" : batch_x,\n",
    "                        \"tf_keepprob:0\" : 1.0,\n",
    "                        self.initial_state: test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state =sess.run(\n",
    "                                        ['probabilities:0', self.final_state],\n",
    "                                        feed_dict =feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict = feed)\n",
    "                \n",
    "                preds.append(pred)\n",
    "            \n",
    "        return np.concatenate(preds)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "   << lstm_output >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "   << final state >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "   << logits >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "   << predictions >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "## Train:\n",
    "n_words = max(list(word_to_int.values()))+1\n",
    "rnn = SentimentRNN(n_words = n_words,\n",
    "                   seq_len = sequence_length,\n",
    "                   embed_size = 256,\n",
    "                   lstm_size = 128,\n",
    "                   num_layers = 1,\n",
    "                   batch_size = 100,\n",
    "                   learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 20| Train loss: 0.66341\n",
      "Epoch: 1/40 Iteration: 40| Train loss: 0.65519\n",
      "Epoch: 1/40 Iteration: 60| Train loss: 0.61008\n",
      "Epoch: 1/40 Iteration: 80| Train loss: 0.57383\n",
      "Epoch: 1/40 Iteration: 100| Train loss: 0.50775\n",
      "Epoch: 1/40 Iteration: 120| Train loss: 0.50454\n",
      "Epoch: 1/40 Iteration: 140| Train loss: 0.53387\n",
      "Epoch: 1/40 Iteration: 160| Train loss: 0.57484\n",
      "Epoch: 1/40 Iteration: 180| Train loss: 0.56980\n",
      "Epoch: 1/40 Iteration: 200| Train loss: 0.50458\n",
      "Epoch: 1/40 Iteration: 220| Train loss: 0.41951\n",
      "Epoch: 1/40 Iteration: 240| Train loss: 0.45895\n",
      "Epoch: 2/40 Iteration: 260| Train loss: 0.33960\n",
      "Epoch: 2/40 Iteration: 280| Train loss: 0.46478\n",
      "Epoch: 2/40 Iteration: 300| Train loss: 0.44123\n",
      "Epoch: 2/40 Iteration: 320| Train loss: 0.30060\n",
      "Epoch: 2/40 Iteration: 340| Train loss: 0.36178\n",
      "Epoch: 2/40 Iteration: 360| Train loss: 0.41756\n",
      "Epoch: 2/40 Iteration: 380| Train loss: 0.36257\n",
      "Epoch: 2/40 Iteration: 400| Train loss: 0.29643\n",
      "Epoch: 2/40 Iteration: 420| Train loss: 0.30601\n",
      "Epoch: 2/40 Iteration: 440| Train loss: 0.31026\n",
      "Epoch: 2/40 Iteration: 460| Train loss: 0.33517\n",
      "Epoch: 2/40 Iteration: 480| Train loss: 0.38265\n",
      "Epoch: 2/40 Iteration: 500| Train loss: 0.21526\n",
      "Epoch: 3/40 Iteration: 520| Train loss: 0.32841\n",
      "Epoch: 3/40 Iteration: 540| Train loss: 0.45193\n",
      "Epoch: 3/40 Iteration: 560| Train loss: 0.23803\n",
      "Epoch: 3/40 Iteration: 580| Train loss: 0.28542\n",
      "Epoch: 3/40 Iteration: 600| Train loss: 0.17255\n",
      "Epoch: 3/40 Iteration: 620| Train loss: 0.30375\n",
      "Epoch: 3/40 Iteration: 640| Train loss: 0.25092\n",
      "Epoch: 3/40 Iteration: 660| Train loss: 0.28873\n",
      "Epoch: 3/40 Iteration: 680| Train loss: 0.24380\n",
      "Epoch: 3/40 Iteration: 700| Train loss: 0.22032\n",
      "Epoch: 3/40 Iteration: 720| Train loss: 0.25918\n",
      "Epoch: 3/40 Iteration: 740| Train loss: 0.35494\n",
      "Epoch: 4/40 Iteration: 760| Train loss: 0.22813\n",
      "Epoch: 4/40 Iteration: 780| Train loss: 0.27560\n",
      "Epoch: 4/40 Iteration: 800| Train loss: 0.22712\n",
      "Epoch: 4/40 Iteration: 820| Train loss: 0.20589\n",
      "Epoch: 4/40 Iteration: 840| Train loss: 0.29677\n",
      "Epoch: 4/40 Iteration: 860| Train loss: 0.28303\n",
      "Epoch: 4/40 Iteration: 880| Train loss: 0.15902\n",
      "Epoch: 4/40 Iteration: 900| Train loss: 0.12199\n",
      "Epoch: 4/40 Iteration: 920| Train loss: 0.11534\n",
      "Epoch: 4/40 Iteration: 940| Train loss: 0.21914\n",
      "Epoch: 4/40 Iteration: 960| Train loss: 0.14638\n",
      "Epoch: 4/40 Iteration: 980| Train loss: 0.31406\n",
      "Epoch: 4/40 Iteration: 1000| Train loss: 0.03888\n",
      "Epoch: 5/40 Iteration: 1020| Train loss: 0.12638\n",
      "Epoch: 5/40 Iteration: 1040| Train loss: 0.25252\n",
      "Epoch: 5/40 Iteration: 1060| Train loss: 0.22184\n",
      "Epoch: 5/40 Iteration: 1080| Train loss: 0.13918\n",
      "Epoch: 5/40 Iteration: 1100| Train loss: 0.07714\n",
      "Epoch: 5/40 Iteration: 1120| Train loss: 0.16471\n",
      "Epoch: 5/40 Iteration: 1140| Train loss: 0.16648\n",
      "Epoch: 5/40 Iteration: 1160| Train loss: 0.15742\n",
      "Epoch: 5/40 Iteration: 1180| Train loss: 0.08595\n",
      "Epoch: 5/40 Iteration: 1200| Train loss: 0.07530\n",
      "Epoch: 5/40 Iteration: 1220| Train loss: 0.13386\n",
      "Epoch: 5/40 Iteration: 1240| Train loss: 0.07941\n",
      "Epoch: 6/40 Iteration: 1260| Train loss: 0.14699\n",
      "Epoch: 6/40 Iteration: 1280| Train loss: 0.13107\n",
      "Epoch: 6/40 Iteration: 1300| Train loss: 0.19576\n",
      "Epoch: 6/40 Iteration: 1320| Train loss: 0.09141\n",
      "Epoch: 6/40 Iteration: 1340| Train loss: 0.15920\n",
      "Epoch: 6/40 Iteration: 1360| Train loss: 0.17031\n",
      "Epoch: 6/40 Iteration: 1380| Train loss: 0.05212\n",
      "Epoch: 6/40 Iteration: 1400| Train loss: 0.05745\n",
      "Epoch: 6/40 Iteration: 1420| Train loss: 0.03852\n",
      "Epoch: 6/40 Iteration: 1440| Train loss: 0.11759\n",
      "Epoch: 6/40 Iteration: 1460| Train loss: 0.15917\n",
      "Epoch: 6/40 Iteration: 1480| Train loss: 0.17496\n",
      "Epoch: 6/40 Iteration: 1500| Train loss: 0.03104\n",
      "Epoch: 7/40 Iteration: 1520| Train loss: 0.13596\n",
      "Epoch: 7/40 Iteration: 1540| Train loss: 0.20693\n",
      "Epoch: 7/40 Iteration: 1560| Train loss: 0.09141\n",
      "Epoch: 7/40 Iteration: 1580| Train loss: 0.07026\n",
      "Epoch: 7/40 Iteration: 1600| Train loss: 0.01242\n",
      "Epoch: 7/40 Iteration: 1620| Train loss: 0.09331\n",
      "Epoch: 7/40 Iteration: 1640| Train loss: 0.20506\n",
      "Epoch: 7/40 Iteration: 1660| Train loss: 0.31431\n",
      "Epoch: 7/40 Iteration: 1680| Train loss: 0.18045\n",
      "Epoch: 7/40 Iteration: 1700| Train loss: 0.16612\n",
      "Epoch: 7/40 Iteration: 1720| Train loss: 0.16961\n",
      "Epoch: 7/40 Iteration: 1740| Train loss: 0.03644\n",
      "Epoch: 8/40 Iteration: 1760| Train loss: 0.09783\n",
      "Epoch: 8/40 Iteration: 1780| Train loss: 0.15676\n",
      "Epoch: 8/40 Iteration: 1800| Train loss: 0.21743\n",
      "Epoch: 8/40 Iteration: 1820| Train loss: 0.10945\n",
      "Epoch: 8/40 Iteration: 1840| Train loss: 0.05736\n",
      "Epoch: 8/40 Iteration: 1860| Train loss: 0.05564\n",
      "Epoch: 8/40 Iteration: 1880| Train loss: 0.02449\n",
      "Epoch: 8/40 Iteration: 1900| Train loss: 0.03060\n",
      "Epoch: 8/40 Iteration: 1920| Train loss: 0.07381\n",
      "Epoch: 8/40 Iteration: 1940| Train loss: 0.04963\n",
      "Epoch: 8/40 Iteration: 1960| Train loss: 0.03070\n",
      "Epoch: 8/40 Iteration: 1980| Train loss: 0.13462\n",
      "Epoch: 8/40 Iteration: 2000| Train loss: 0.00596\n",
      "Epoch: 9/40 Iteration: 2020| Train loss: 0.08164\n",
      "Epoch: 9/40 Iteration: 2040| Train loss: 0.02362\n",
      "Epoch: 9/40 Iteration: 2060| Train loss: 0.02907\n",
      "Epoch: 9/40 Iteration: 2080| Train loss: 0.08676\n",
      "Epoch: 9/40 Iteration: 2100| Train loss: 0.01491\n",
      "Epoch: 9/40 Iteration: 2120| Train loss: 0.00962\n",
      "Epoch: 9/40 Iteration: 2140| Train loss: 0.03251\n",
      "Epoch: 9/40 Iteration: 2160| Train loss: 0.03597\n",
      "Epoch: 9/40 Iteration: 2180| Train loss: 0.01651\n",
      "Epoch: 9/40 Iteration: 2200| Train loss: 0.00337\n",
      "Epoch: 9/40 Iteration: 2220| Train loss: 0.00608\n",
      "Epoch: 9/40 Iteration: 2240| Train loss: 0.00347\n",
      "Epoch: 10/40 Iteration: 2260| Train loss: 0.00967\n",
      "Epoch: 10/40 Iteration: 2280| Train loss: 0.01729\n",
      "Epoch: 10/40 Iteration: 2300| Train loss: 0.02971\n",
      "Epoch: 10/40 Iteration: 2320| Train loss: 0.10799\n",
      "Epoch: 10/40 Iteration: 2340| Train loss: 0.01167\n",
      "Epoch: 10/40 Iteration: 2360| Train loss: 0.05067\n",
      "Epoch: 10/40 Iteration: 2380| Train loss: 0.00497\n",
      "Epoch: 10/40 Iteration: 2400| Train loss: 0.00824\n",
      "Epoch: 10/40 Iteration: 2420| Train loss: 0.00199\n",
      "Epoch: 10/40 Iteration: 2440| Train loss: 0.00986\n",
      "Epoch: 10/40 Iteration: 2460| Train loss: 0.00665\n",
      "Epoch: 10/40 Iteration: 2480| Train loss: 0.06408\n",
      "Epoch: 10/40 Iteration: 2500| Train loss: 0.00936\n",
      "Epoch: 11/40 Iteration: 2520| Train loss: 0.01094\n",
      "Epoch: 11/40 Iteration: 2540| Train loss: 0.00377\n",
      "Epoch: 11/40 Iteration: 2560| Train loss: 0.00139\n",
      "Epoch: 11/40 Iteration: 2580| Train loss: 0.04908\n",
      "Epoch: 11/40 Iteration: 2600| Train loss: 0.00343\n",
      "Epoch: 11/40 Iteration: 2620| Train loss: 0.03680\n",
      "Epoch: 11/40 Iteration: 2640| Train loss: 0.00719\n",
      "Epoch: 11/40 Iteration: 2660| Train loss: 0.00873\n",
      "Epoch: 11/40 Iteration: 2680| Train loss: 0.00638\n",
      "Epoch: 11/40 Iteration: 2700| Train loss: 0.00127\n",
      "Epoch: 11/40 Iteration: 2720| Train loss: 0.00915\n",
      "Epoch: 11/40 Iteration: 2740| Train loss: 0.01797\n",
      "Epoch: 12/40 Iteration: 2760| Train loss: 0.05704\n",
      "Epoch: 12/40 Iteration: 2780| Train loss: 0.08655\n",
      "Epoch: 12/40 Iteration: 2800| Train loss: 0.00787\n",
      "Epoch: 12/40 Iteration: 2820| Train loss: 0.01404\n",
      "Epoch: 12/40 Iteration: 2840| Train loss: 0.02445\n",
      "Epoch: 12/40 Iteration: 2860| Train loss: 0.01037\n",
      "Epoch: 12/40 Iteration: 2880| Train loss: 0.00475\n",
      "Epoch: 12/40 Iteration: 2900| Train loss: 0.02299\n",
      "Epoch: 12/40 Iteration: 2920| Train loss: 0.00132\n",
      "Epoch: 12/40 Iteration: 2940| Train loss: 0.02553\n",
      "Epoch: 12/40 Iteration: 2960| Train loss: 0.02547\n",
      "Epoch: 12/40 Iteration: 2980| Train loss: 0.06571\n",
      "Epoch: 12/40 Iteration: 3000| Train loss: 0.00776\n",
      "Epoch: 13/40 Iteration: 3020| Train loss: 0.00866\n",
      "Epoch: 13/40 Iteration: 3040| Train loss: 0.03135\n",
      "Epoch: 13/40 Iteration: 3060| Train loss: 0.00578\n",
      "Epoch: 13/40 Iteration: 3080| Train loss: 0.01055\n",
      "Epoch: 13/40 Iteration: 3100| Train loss: 0.00246\n",
      "Epoch: 13/40 Iteration: 3120| Train loss: 0.00236\n",
      "Epoch: 13/40 Iteration: 3140| Train loss: 0.00754\n",
      "Epoch: 13/40 Iteration: 3160| Train loss: 0.02157\n",
      "Epoch: 13/40 Iteration: 3180| Train loss: 0.00460\n",
      "Epoch: 13/40 Iteration: 3200| Train loss: 0.02808\n",
      "Epoch: 13/40 Iteration: 3220| Train loss: 0.00514\n",
      "Epoch: 13/40 Iteration: 3240| Train loss: 0.00489\n",
      "Epoch: 14/40 Iteration: 3260| Train loss: 0.02673\n",
      "Epoch: 14/40 Iteration: 3280| Train loss: 0.01540\n",
      "Epoch: 14/40 Iteration: 3300| Train loss: 0.03128\n",
      "Epoch: 14/40 Iteration: 3320| Train loss: 0.01154\n",
      "Epoch: 14/40 Iteration: 3340| Train loss: 0.00328\n",
      "Epoch: 14/40 Iteration: 3360| Train loss: 0.00658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 Iteration: 3380| Train loss: 0.00343\n",
      "Epoch: 14/40 Iteration: 3400| Train loss: 0.00084\n",
      "Epoch: 14/40 Iteration: 3420| Train loss: 0.00080\n",
      "Epoch: 14/40 Iteration: 3440| Train loss: 0.00358\n",
      "Epoch: 14/40 Iteration: 3460| Train loss: 0.01215\n",
      "Epoch: 14/40 Iteration: 3480| Train loss: 0.00451\n",
      "Epoch: 14/40 Iteration: 3500| Train loss: 0.01979\n",
      "Epoch: 15/40 Iteration: 3520| Train loss: 0.00519\n",
      "Epoch: 15/40 Iteration: 3540| Train loss: 0.02459\n",
      "Epoch: 15/40 Iteration: 3560| Train loss: 0.01432\n",
      "Epoch: 15/40 Iteration: 3580| Train loss: 0.00702\n",
      "Epoch: 15/40 Iteration: 3600| Train loss: 0.00224\n",
      "Epoch: 15/40 Iteration: 3620| Train loss: 0.01107\n",
      "Epoch: 15/40 Iteration: 3640| Train loss: 0.01135\n",
      "Epoch: 15/40 Iteration: 3660| Train loss: 0.01072\n",
      "Epoch: 15/40 Iteration: 3680| Train loss: 0.01060\n",
      "Epoch: 15/40 Iteration: 3700| Train loss: 0.00217\n",
      "Epoch: 15/40 Iteration: 3720| Train loss: 0.03234\n",
      "Epoch: 15/40 Iteration: 3740| Train loss: 0.00333\n",
      "Epoch: 16/40 Iteration: 3760| Train loss: 0.00654\n",
      "Epoch: 16/40 Iteration: 3780| Train loss: 0.00130\n",
      "Epoch: 16/40 Iteration: 3800| Train loss: 0.00139\n",
      "Epoch: 16/40 Iteration: 3820| Train loss: 0.06588\n",
      "Epoch: 16/40 Iteration: 3840| Train loss: 0.02509\n",
      "Epoch: 16/40 Iteration: 3860| Train loss: 0.01915\n",
      "Epoch: 16/40 Iteration: 3880| Train loss: 0.04580\n",
      "Epoch: 16/40 Iteration: 3900| Train loss: 0.00476\n",
      "Epoch: 16/40 Iteration: 3920| Train loss: 0.02963\n",
      "Epoch: 16/40 Iteration: 3940| Train loss: 0.05741\n",
      "Epoch: 16/40 Iteration: 3960| Train loss: 0.07381\n",
      "Epoch: 16/40 Iteration: 3980| Train loss: 0.01379\n",
      "Epoch: 16/40 Iteration: 4000| Train loss: 0.02354\n",
      "Epoch: 17/40 Iteration: 4020| Train loss: 0.00452\n",
      "Epoch: 17/40 Iteration: 4040| Train loss: 0.03466\n",
      "Epoch: 17/40 Iteration: 4060| Train loss: 0.01937\n",
      "Epoch: 17/40 Iteration: 4080| Train loss: 0.00176\n",
      "Epoch: 17/40 Iteration: 4100| Train loss: 0.01694\n",
      "Epoch: 17/40 Iteration: 4120| Train loss: 0.00211\n",
      "Epoch: 17/40 Iteration: 4140| Train loss: 0.01472\n",
      "Epoch: 17/40 Iteration: 4160| Train loss: 0.02979\n",
      "Epoch: 17/40 Iteration: 4180| Train loss: 0.00416\n",
      "Epoch: 17/40 Iteration: 4200| Train loss: 0.00132\n",
      "Epoch: 17/40 Iteration: 4220| Train loss: 0.00147\n",
      "Epoch: 17/40 Iteration: 4240| Train loss: 0.00316\n",
      "Epoch: 18/40 Iteration: 4260| Train loss: 0.00645\n",
      "Epoch: 18/40 Iteration: 4280| Train loss: 0.00450\n",
      "Epoch: 18/40 Iteration: 4300| Train loss: 0.00662\n",
      "Epoch: 18/40 Iteration: 4320| Train loss: 0.00536\n",
      "Epoch: 18/40 Iteration: 4340| Train loss: 0.00171\n",
      "Epoch: 18/40 Iteration: 4360| Train loss: 0.02558\n",
      "Epoch: 18/40 Iteration: 4380| Train loss: 0.00070\n",
      "Epoch: 18/40 Iteration: 4400| Train loss: 0.00101\n",
      "Epoch: 18/40 Iteration: 4420| Train loss: 0.00085\n",
      "Epoch: 18/40 Iteration: 4440| Train loss: 0.00287\n",
      "Epoch: 18/40 Iteration: 4460| Train loss: 0.00075\n",
      "Epoch: 18/40 Iteration: 4480| Train loss: 0.00037\n",
      "Epoch: 18/40 Iteration: 4500| Train loss: 0.00173\n",
      "Epoch: 19/40 Iteration: 4520| Train loss: 0.00037\n",
      "Epoch: 19/40 Iteration: 4540| Train loss: 0.00323\n",
      "Epoch: 19/40 Iteration: 4560| Train loss: 0.00126\n",
      "Epoch: 19/40 Iteration: 4580| Train loss: 0.00014\n",
      "Epoch: 19/40 Iteration: 4600| Train loss: 0.00193\n",
      "Epoch: 19/40 Iteration: 4620| Train loss: 0.00226\n",
      "Epoch: 19/40 Iteration: 4640| Train loss: 0.00090\n",
      "Epoch: 19/40 Iteration: 4660| Train loss: 0.00040\n",
      "Epoch: 19/40 Iteration: 4680| Train loss: 0.00511\n",
      "Epoch: 19/40 Iteration: 4700| Train loss: 0.00011\n",
      "Epoch: 19/40 Iteration: 4720| Train loss: 0.00010\n",
      "Epoch: 19/40 Iteration: 4740| Train loss: 0.00014\n",
      "Epoch: 20/40 Iteration: 4760| Train loss: 0.00347\n",
      "Epoch: 20/40 Iteration: 4780| Train loss: 0.00247\n",
      "Epoch: 20/40 Iteration: 4800| Train loss: 0.00041\n",
      "Epoch: 20/40 Iteration: 4820| Train loss: 0.00912\n",
      "Epoch: 20/40 Iteration: 4840| Train loss: 0.00539\n",
      "Epoch: 20/40 Iteration: 4860| Train loss: 0.00374\n",
      "Epoch: 20/40 Iteration: 4880| Train loss: 0.00131\n",
      "Epoch: 20/40 Iteration: 4900| Train loss: 0.00238\n",
      "Epoch: 20/40 Iteration: 4920| Train loss: 0.00045\n",
      "Epoch: 20/40 Iteration: 4940| Train loss: 0.00186\n",
      "Epoch: 20/40 Iteration: 4960| Train loss: 0.00156\n",
      "Epoch: 20/40 Iteration: 4980| Train loss: 0.00192\n",
      "Epoch: 20/40 Iteration: 5000| Train loss: 0.00079\n",
      "Epoch: 21/40 Iteration: 5020| Train loss: 0.02897\n",
      "Epoch: 21/40 Iteration: 5040| Train loss: 0.00248\n",
      "Epoch: 21/40 Iteration: 5060| Train loss: 0.02507\n",
      "Epoch: 21/40 Iteration: 5080| Train loss: 0.00482\n",
      "Epoch: 21/40 Iteration: 5100| Train loss: 0.00186\n",
      "Epoch: 21/40 Iteration: 5120| Train loss: 0.00126\n",
      "Epoch: 21/40 Iteration: 5140| Train loss: 0.00186\n",
      "Epoch: 21/40 Iteration: 5160| Train loss: 0.00525\n",
      "Epoch: 21/40 Iteration: 5180| Train loss: 0.01926\n",
      "Epoch: 21/40 Iteration: 5200| Train loss: 0.02248\n",
      "Epoch: 21/40 Iteration: 5220| Train loss: 0.00044\n",
      "Epoch: 21/40 Iteration: 5240| Train loss: 0.00288\n",
      "Epoch: 22/40 Iteration: 5260| Train loss: 0.00294\n",
      "Epoch: 22/40 Iteration: 5280| Train loss: 0.01938\n",
      "Epoch: 22/40 Iteration: 5300| Train loss: 0.00088\n",
      "Epoch: 22/40 Iteration: 5320| Train loss: 0.00133\n",
      "Epoch: 22/40 Iteration: 5340| Train loss: 0.00660\n",
      "Epoch: 22/40 Iteration: 5360| Train loss: 0.00331\n",
      "Epoch: 22/40 Iteration: 5380| Train loss: 0.00053\n",
      "Epoch: 22/40 Iteration: 5400| Train loss: 0.00318\n",
      "Epoch: 22/40 Iteration: 5420| Train loss: 0.00117\n",
      "Epoch: 22/40 Iteration: 5440| Train loss: 0.00275\n",
      "Epoch: 22/40 Iteration: 5460| Train loss: 0.00063\n",
      "Epoch: 22/40 Iteration: 5480| Train loss: 0.00300\n",
      "Epoch: 22/40 Iteration: 5500| Train loss: 0.00565\n",
      "Epoch: 23/40 Iteration: 5520| Train loss: 0.04045\n",
      "Epoch: 23/40 Iteration: 5540| Train loss: 0.01135\n",
      "Epoch: 23/40 Iteration: 5560| Train loss: 0.00912\n",
      "Epoch: 23/40 Iteration: 5580| Train loss: 0.07307\n",
      "Epoch: 23/40 Iteration: 5600| Train loss: 0.00125\n",
      "Epoch: 23/40 Iteration: 5620| Train loss: 0.00187\n",
      "Epoch: 23/40 Iteration: 5640| Train loss: 0.00343\n",
      "Epoch: 23/40 Iteration: 5660| Train loss: 0.00514\n",
      "Epoch: 23/40 Iteration: 5680| Train loss: 0.00293\n",
      "Epoch: 23/40 Iteration: 5700| Train loss: 0.01160\n",
      "Epoch: 23/40 Iteration: 5720| Train loss: 0.00183\n",
      "Epoch: 23/40 Iteration: 5740| Train loss: 0.10874\n",
      "Epoch: 24/40 Iteration: 5760| Train loss: 0.00870\n",
      "Epoch: 24/40 Iteration: 5780| Train loss: 0.00587\n",
      "Epoch: 24/40 Iteration: 5800| Train loss: 0.00088\n",
      "Epoch: 24/40 Iteration: 5820| Train loss: 0.00129\n",
      "Epoch: 24/40 Iteration: 5840| Train loss: 0.00113\n",
      "Epoch: 24/40 Iteration: 5860| Train loss: 0.01673\n",
      "Epoch: 24/40 Iteration: 5880| Train loss: 0.00060\n",
      "Epoch: 24/40 Iteration: 5900| Train loss: 0.00082\n",
      "Epoch: 24/40 Iteration: 5920| Train loss: 0.00060\n",
      "Epoch: 24/40 Iteration: 5940| Train loss: 0.00483\n",
      "Epoch: 24/40 Iteration: 5960| Train loss: 0.00121\n",
      "Epoch: 24/40 Iteration: 5980| Train loss: 0.00973\n",
      "Epoch: 24/40 Iteration: 6000| Train loss: 0.00034\n",
      "Epoch: 25/40 Iteration: 6020| Train loss: 0.00147\n",
      "Epoch: 25/40 Iteration: 6040| Train loss: 0.00028\n",
      "Epoch: 25/40 Iteration: 6060| Train loss: 0.00266\n",
      "Epoch: 25/40 Iteration: 6080| Train loss: 0.00016\n",
      "Epoch: 25/40 Iteration: 6100| Train loss: 0.00015\n",
      "Epoch: 25/40 Iteration: 6120| Train loss: 0.01453\n",
      "Epoch: 25/40 Iteration: 6140| Train loss: 0.00162\n",
      "Epoch: 25/40 Iteration: 6160| Train loss: 0.01257\n",
      "Epoch: 25/40 Iteration: 6180| Train loss: 0.00199\n",
      "Epoch: 25/40 Iteration: 6200| Train loss: 0.00011\n",
      "Epoch: 25/40 Iteration: 6220| Train loss: 0.00126\n",
      "Epoch: 25/40 Iteration: 6240| Train loss: 0.00064\n",
      "Epoch: 26/40 Iteration: 6260| Train loss: 0.00030\n",
      "Epoch: 26/40 Iteration: 6280| Train loss: 0.00018\n",
      "Epoch: 26/40 Iteration: 6300| Train loss: 0.00009\n",
      "Epoch: 26/40 Iteration: 6320| Train loss: 0.00031\n",
      "Epoch: 26/40 Iteration: 6340| Train loss: 0.00080\n",
      "Epoch: 26/40 Iteration: 6360| Train loss: 0.00065\n",
      "Epoch: 26/40 Iteration: 6380| Train loss: 0.00035\n",
      "Epoch: 26/40 Iteration: 6400| Train loss: 0.00013\n",
      "Epoch: 26/40 Iteration: 6420| Train loss: 0.00014\n",
      "Epoch: 26/40 Iteration: 6440| Train loss: 0.00026\n",
      "Epoch: 26/40 Iteration: 6460| Train loss: 0.00014\n",
      "Epoch: 26/40 Iteration: 6480| Train loss: 0.00043\n",
      "Epoch: 26/40 Iteration: 6500| Train loss: 0.00015\n",
      "Epoch: 27/40 Iteration: 6520| Train loss: 0.00011\n",
      "Epoch: 27/40 Iteration: 6540| Train loss: 0.00007\n",
      "Epoch: 27/40 Iteration: 6560| Train loss: 0.00047\n",
      "Epoch: 27/40 Iteration: 6580| Train loss: 0.00008\n",
      "Epoch: 27/40 Iteration: 6600| Train loss: 0.00025\n",
      "Epoch: 27/40 Iteration: 6620| Train loss: 0.00011\n",
      "Epoch: 27/40 Iteration: 6640| Train loss: 0.00045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40 Iteration: 6660| Train loss: 0.00018\n",
      "Epoch: 27/40 Iteration: 6680| Train loss: 0.00041\n",
      "Epoch: 27/40 Iteration: 6700| Train loss: 0.00004\n",
      "Epoch: 27/40 Iteration: 6720| Train loss: 0.00006\n",
      "Epoch: 27/40 Iteration: 6740| Train loss: 0.00008\n",
      "Epoch: 28/40 Iteration: 6760| Train loss: 0.00009\n",
      "Epoch: 28/40 Iteration: 6780| Train loss: 0.00010\n",
      "Epoch: 28/40 Iteration: 6800| Train loss: 0.00005\n",
      "Epoch: 28/40 Iteration: 6820| Train loss: 0.00030\n",
      "Epoch: 28/40 Iteration: 6840| Train loss: 0.00011\n",
      "Epoch: 28/40 Iteration: 6860| Train loss: 0.00024\n",
      "Epoch: 28/40 Iteration: 6880| Train loss: 0.00006\n",
      "Epoch: 28/40 Iteration: 6900| Train loss: 0.00018\n",
      "Epoch: 28/40 Iteration: 6920| Train loss: 0.00005\n",
      "Epoch: 28/40 Iteration: 6940| Train loss: 0.00005\n",
      "Epoch: 28/40 Iteration: 6960| Train loss: 0.00008\n",
      "Epoch: 28/40 Iteration: 6980| Train loss: 0.00002\n",
      "Epoch: 28/40 Iteration: 7000| Train loss: 0.00007\n",
      "Epoch: 29/40 Iteration: 7020| Train loss: 0.00007\n",
      "Epoch: 29/40 Iteration: 7040| Train loss: 0.00003\n",
      "Epoch: 29/40 Iteration: 7060| Train loss: 0.00004\n",
      "Epoch: 29/40 Iteration: 7080| Train loss: 0.00006\n",
      "Epoch: 29/40 Iteration: 7100| Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7120| Train loss: 0.00007\n",
      "Epoch: 29/40 Iteration: 7140| Train loss: 0.00074\n",
      "Epoch: 29/40 Iteration: 7160| Train loss: 0.00008\n",
      "Epoch: 29/40 Iteration: 7180| Train loss: 0.00006\n",
      "Epoch: 29/40 Iteration: 7200| Train loss: 0.00003\n",
      "Epoch: 29/40 Iteration: 7220| Train loss: 0.00010\n",
      "Epoch: 29/40 Iteration: 7240| Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7260| Train loss: 0.00006\n",
      "Epoch: 30/40 Iteration: 7280| Train loss: 0.00006\n",
      "Epoch: 30/40 Iteration: 7300| Train loss: 0.00013\n",
      "Epoch: 30/40 Iteration: 7320| Train loss: 0.00007\n",
      "Epoch: 30/40 Iteration: 7340| Train loss: 0.00013\n",
      "Epoch: 30/40 Iteration: 7360| Train loss: 0.00005\n",
      "Epoch: 30/40 Iteration: 7380| Train loss: 0.00006\n",
      "Epoch: 30/40 Iteration: 7400| Train loss: 0.00001\n",
      "Epoch: 30/40 Iteration: 7420| Train loss: 0.00007\n",
      "Epoch: 30/40 Iteration: 7440| Train loss: 0.00003\n",
      "Epoch: 30/40 Iteration: 7460| Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7480| Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7500| Train loss: 0.00015\n",
      "Epoch: 31/40 Iteration: 7520| Train loss: 0.00003\n",
      "Epoch: 31/40 Iteration: 7540| Train loss: 0.00003\n",
      "Epoch: 31/40 Iteration: 7560| Train loss: 0.00004\n",
      "Epoch: 31/40 Iteration: 7580| Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7600| Train loss: 0.00007\n",
      "Epoch: 31/40 Iteration: 7620| Train loss: 0.00005\n",
      "Epoch: 31/40 Iteration: 7640| Train loss: 0.00015\n",
      "Epoch: 31/40 Iteration: 7660| Train loss: 0.00010\n",
      "Epoch: 31/40 Iteration: 7680| Train loss: 0.00021\n",
      "Epoch: 31/40 Iteration: 7700| Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7720| Train loss: 0.00006\n",
      "Epoch: 31/40 Iteration: 7740| Train loss: 0.00015\n",
      "Epoch: 32/40 Iteration: 7760| Train loss: 0.00008\n",
      "Epoch: 32/40 Iteration: 7780| Train loss: 0.00005\n",
      "Epoch: 32/40 Iteration: 7800| Train loss: 0.00004\n",
      "Epoch: 32/40 Iteration: 7820| Train loss: 0.00011\n",
      "Epoch: 32/40 Iteration: 7840| Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7860| Train loss: 0.00009\n",
      "Epoch: 32/40 Iteration: 7880| Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7900| Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7920| Train loss: 0.00006\n",
      "Epoch: 32/40 Iteration: 7940| Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7960| Train loss: 0.00003\n",
      "Epoch: 32/40 Iteration: 7980| Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 8000| Train loss: 0.00002\n",
      "Epoch: 33/40 Iteration: 8020| Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8040| Train loss: 0.00000\n",
      "Epoch: 33/40 Iteration: 8060| Train loss: 0.00003\n",
      "Epoch: 33/40 Iteration: 8080| Train loss: 0.00003\n",
      "Epoch: 33/40 Iteration: 8100| Train loss: 0.00002\n",
      "Epoch: 33/40 Iteration: 8120| Train loss: 0.00003\n",
      "Epoch: 33/40 Iteration: 8140| Train loss: 0.00012\n",
      "Epoch: 33/40 Iteration: 8160| Train loss: 0.00005\n",
      "Epoch: 33/40 Iteration: 8180| Train loss: 0.00004\n",
      "Epoch: 33/40 Iteration: 8200| Train loss: 0.00000\n",
      "Epoch: 33/40 Iteration: 8220| Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8240| Train loss: 0.00000\n",
      "Epoch: 34/40 Iteration: 8260| Train loss: 0.00022\n",
      "Epoch: 34/40 Iteration: 8280| Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8300| Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8320| Train loss: 0.00012\n",
      "Epoch: 34/40 Iteration: 8340| Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8360| Train loss: 0.00005\n",
      "Epoch: 34/40 Iteration: 8380| Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8400| Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8420| Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8440| Train loss: 0.00005\n",
      "Epoch: 34/40 Iteration: 8460| Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8480| Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8500| Train loss: 0.00003\n",
      "Epoch: 35/40 Iteration: 8520| Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8540| Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8560| Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8580| Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8600| Train loss: 0.00004\n",
      "Epoch: 35/40 Iteration: 8620| Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8640| Train loss: 0.00002\n",
      "Epoch: 35/40 Iteration: 8660| Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8680| Train loss: 0.00006\n",
      "Epoch: 35/40 Iteration: 8700| Train loss: 0.00002\n",
      "Epoch: 35/40 Iteration: 8720| Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8740| Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8760| Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8780| Train loss: 0.00006\n",
      "Epoch: 36/40 Iteration: 8800| Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8820| Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8840| Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8860| Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8880| Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8900| Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8920| Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8940| Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8960| Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8980| Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 9000| Train loss: 0.00003\n",
      "Epoch: 37/40 Iteration: 9020| Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9040| Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9060| Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9080| Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9100| Train loss: 0.00006\n",
      "Epoch: 37/40 Iteration: 9120| Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9140| Train loss: 0.00003\n",
      "Epoch: 37/40 Iteration: 9160| Train loss: 0.00005\n",
      "Epoch: 37/40 Iteration: 9180| Train loss: 0.00005\n",
      "Epoch: 37/40 Iteration: 9200| Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9220| Train loss: 0.00002\n",
      "Epoch: 37/40 Iteration: 9240| Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9260| Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9280| Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9300| Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9320| Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9340| Train loss: 0.00000\n",
      "Epoch: 38/40 Iteration: 9360| Train loss: 0.00004\n",
      "Epoch: 38/40 Iteration: 9380| Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9400| Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9420| Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9440| Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9460| Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9480| Train loss: 0.00000\n",
      "Epoch: 38/40 Iteration: 9500| Train loss: 0.00002\n",
      "Epoch: 39/40 Iteration: 9520| Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9540| Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9560| Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9580| Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9600| Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9620| Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9640| Train loss: 0.00005\n",
      "Epoch: 39/40 Iteration: 9660| Train loss: 0.00002\n",
      "Epoch: 39/40 Iteration: 9680| Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9700| Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9720| Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9740| Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9760| Train loss: 0.00003\n",
      "Epoch: 40/40 Iteration: 9780| Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9800| Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9820| Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9840| Train loss: 0.00002\n",
      "Epoch: 40/40 Iteration: 9860| Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9880| Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9900| Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9920| Train loss: 0.00002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/40 Iteration: 9940| Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9960| Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9980| Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 10000| Train loss: 0.00002\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/sentiment-39.ckpt\n",
      "Test Acc.: 0.854\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % ( np.sum(preds == y_true)/len(y_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/sentiment-39.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Get probabilities\n",
    "proba = rnn.predict(X_test, return_proba = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: Implementing an RNN for character-level language modeling in TensorFlow\n",
    "\n",
    "Input: a text document\n",
    "Goal: develop a model that can generate new text similar to the input document\n",
    "\n",
    "In character-level langague modeling, the input is broken down into a sequence of characters that are fed into our network one character at a time.\n",
    "\n",
    "The network will process each new character in conjuction with the memory of the previously seen characters to predict the next character\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1) Preparing the data\n",
    "\n",
    "2) Building the RNN model\n",
    "\n",
    "3) Performing next-character prediction and sampling to generate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "## reading and processing text\n",
    "with open('pg2265.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[15858:] # remove the beginning portion of the text that contains some legal description of the Gutenberg project\n",
    "chars = set(text)\n",
    "char2int = {ch: i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text],\n",
    "                     dtype = np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data into batches of sequences\n",
    "\n",
    "shift the input(x) and output(y) of the neural network by one character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 31 63 25 15 50 57  8 63 53]\n",
      "[31 63 25 15 50 57  8 63 53 37]\n",
      "The Tragedie of Hamlet\n",
      "\n",
      "Actus Primus. Scoena Prima\n"
     ]
    }
   ],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    tot_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence)/tot_batch_length)\n",
    "    \n",
    "    if num_batches * tot_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    ## Truncate the sequence at the end to get rid of\n",
    "    ## remaining characters that do not make a full batch\n",
    "    x = sequence[0: num_batches * tot_batch_length]\n",
    "    y = sequence[1: num_batches * tot_batch_length + 1]\n",
    "    \n",
    "    ## Split x & y into a list batches of sequences:\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    \n",
    "    ## Stack the batches together\n",
    "    ## batch_size x mini_batch_length\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x,y\n",
    "## Testing:\n",
    "train_x, train_y = reshape_data(text_ints,64,10)\n",
    "print(train_x[0,:10])\n",
    "print(train_y[0,:10])\n",
    "print(''.join(int2char[i] for i in train_x[0,:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15) (64, 15) The Tragedie of      he Tragedie of \n",
      "(64, 15) (64, 15)  Hamlet**Actus       Hamlet**Actus P\n",
      "(64, 15) (64, 15) Primus. Scoena       rimus. Scoena P\n",
      "(64, 15) (64, 15) Prima.**Enter B      rima.**Enter Ba\n",
      "(64, 15) (64, 15) arnardo and Fra      rnardo and Fran\n",
      "(64, 15) (64, 15) ncisco two Cent      cisco two Centi\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    \n",
    "    for b in range(num_batches):\n",
    "        yield(data_x[:,b*num_steps:(b+1)*num_steps],\n",
    "              data_y[:,b*num_steps:(b+1)*num_steps])\n",
    "        \n",
    "bgen = create_batch_generator(train_x[:,:100],train_y[:,:100],15)\n",
    "\n",
    "for b in bgen:\n",
    "    print(b[0].shape,b[1].shape,end=' ')\n",
    "    print(''.join(int2char[i] for i in b[0][0,:]).replace('\\n','*'),'    ',\n",
    "          ''.join(int2char[i] for i in b[1][0,:]).replace('\\n','*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the character-level RNN model\n",
    "\n",
    "- Constructor:\n",
    "    - set up the learning parameters\n",
    "    - create a computation graph\n",
    "    - call the `build` method to construct the graph based on the sampling mode v.s. training mode\n",
    "\n",
    "- `build` method:\n",
    "    - define the placeholders for feeding the data\n",
    "    - construct the RNN using LSTM cells\n",
    "    - define the output of the network\n",
    "    - define the cost function\n",
    "    - define the optimizer\n",
    "    \n",
    "- `train` method:\n",
    "    - iterate through the mini-batches\n",
    "    - train the network for the specified number of epochs\n",
    "\n",
    "- `sample` method:\n",
    "    - calculate the prob. of the next character\n",
    "    - choose a character randomly according to these probs.\n",
    "    - concatenated together to form a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n = 5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size,1,p = p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size = 64,\n",
    "                 num_steps = 100, lstm_size = 128,\n",
    "                 num_layers = 1, learning_rate = 0.001,\n",
    "                 keep_prob = 0.5, grad_clip = 5,\n",
    "                 sampling = False):\n",
    "        ## 1. set up learning parameters\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        ## 2. create computation graph\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            ## 3. calls the `build` method the construct the graph\n",
    "            self.build(sampling = sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1,1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "            \n",
    "        ## 1. define the placeholders for feeding the data\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                              shape = [batch_size, num_steps],\n",
    "                              name = 'tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32,\n",
    "                              shape = [batch_size, num_steps],\n",
    "                              name = 'tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                     name = 'tf_keepprob')\n",
    "        \n",
    "        # One-hot encoding:\n",
    "        x_onehot = tf.one_hot(tf_x, depth = self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth = self.num_classes)\n",
    "        \n",
    "        ## 2. Build the multi-layer RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                [tf.contrib.rnn.DropoutWrapper(\n",
    "                    tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                    output_keep_prob = tf_keepprob)\n",
    "                 for _ in range(self.num_layers)])\n",
    "        \n",
    "        # Define the initial state\n",
    "        self.initial_state = cells.zero_state(\n",
    "                batch_size, tf.float32)\n",
    "        \n",
    "        ## 3. Define the output of the network\n",
    "        # Run each sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                    cells, x_onehot,\n",
    "                    initial_state = self.initial_state)\n",
    "        \n",
    "        print('  <<lstm_outputs   >>', lstm_outputs)\n",
    "        \n",
    "        seq_output_reshaped = tf.reshape(\n",
    "                            lstm_outputs,\n",
    "                            shape = [-1, self.lstm_size],\n",
    "                            name = 'seq_output_reshaped')\n",
    "        \n",
    "        \n",
    "        logits = tf.layers.dense(\n",
    "                    inputs = seq_output_reshaped,\n",
    "                    units =self.num_classes,\n",
    "                    activation = None,\n",
    "                    name = 'logits')\n",
    "        \n",
    "        proba = tf.nn.softmax(logits,\n",
    "                              name = 'probabilities')\n",
    "        \n",
    "        print(proba)\n",
    "        \n",
    "        y_reshaped = tf.reshape(\n",
    "                        y_onehot,\n",
    "                        shape = [-1, self.num_classes],\n",
    "                        name = 'y_reshaped')\n",
    "        \n",
    "        ## 4. Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits = logits,\n",
    "                        labels = y_reshaped),\n",
    "                    name = 'cost')\n",
    "        \n",
    "        # Gradient clipping to avoid \"exploding gradient\"\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "                        tf.gradients(cost, tvars),\n",
    "                        self.grad_clip)\n",
    "        \n",
    "        ## 5. Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),\n",
    "                    name = 'train_op')\n",
    "        \n",
    "        \n",
    "    def train(self, train_x, train_y,\n",
    "              num_epochs, ckpt_dir = './model/'):\n",
    "        ## Create the checkpoint directory\n",
    "        ## if not exisit\n",
    "        \n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            n_batches = int(train_x.shape[1]/self.num_steps)\n",
    "            \n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                # Train network\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                # Minibatch generator\n",
    "                bgen = create_batch_generator(\n",
    "                        train_x, train_y, self.num_steps)\n",
    "                \n",
    "                for b, (batch_x, batch_y) in enumerate(bgen,1):\n",
    "                    \n",
    "                    iteration = epoch * n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0':self.keep_prob,\n",
    "                            self.initial_state : new_state}\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                            ['cost:0', 'train_op',self.final_state],\n",
    "                            feed_dict = feed)\n",
    "                    \n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d'\n",
    "                              '| Training loss: % .4f' %(\n",
    "                              epoch + 1, num_epochs,\n",
    "                              iteration, batch_cost))\n",
    "                        \n",
    "            ## save the trained model\n",
    "            self.saver.save(\n",
    "                    sess, os.path.join(\n",
    "                        ckpt_dir, 'language_modeling.ckpt'))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def sample(self, output_length,\n",
    "               ckpt_dir, starter_seq = 'The '):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess,\n",
    "                tf.train.latest_checkpoint(ckpt_dir))\n",
    "\n",
    "            ## 1. run the model using the starter sequence\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1,1))\n",
    "                x[0,0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict = feed)\n",
    "\n",
    "            ch_id = get_top_char(proba,len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "\n",
    "\n",
    "            ## 2: run the model using the updated observed_sq\n",
    "            for i in range(output_length):\n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0':x,\n",
    "                        'tf_keepprob:0' : 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict = feed)\n",
    "                ch_id  = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training the CharRNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <<lstm_outputs   >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(6400, 65), dtype=float32)\n",
      "Epoch 1/100 Iteration 10| Training loss:  3.7578\n",
      "Epoch 1/100 Iteration 20| Training loss:  3.3869\n",
      "Epoch 2/100 Iteration 30| Training loss:  3.3011\n",
      "Epoch 2/100 Iteration 40| Training loss:  3.2389\n",
      "Epoch 2/100 Iteration 50| Training loss:  3.2294\n",
      "Epoch 3/100 Iteration 60| Training loss:  3.2202\n",
      "Epoch 3/100 Iteration 70| Training loss:  3.1929\n",
      "Epoch 4/100 Iteration 80| Training loss:  3.1841\n",
      "Epoch 4/100 Iteration 90| Training loss:  3.1605\n",
      "Epoch 4/100 Iteration 100| Training loss:  3.1537\n",
      "Epoch 5/100 Iteration 110| Training loss:  3.1390\n",
      "Epoch 5/100 Iteration 120| Training loss:  3.1000\n",
      "Epoch 6/100 Iteration 130| Training loss:  3.0778\n",
      "Epoch 6/100 Iteration 140| Training loss:  3.0434\n",
      "Epoch 6/100 Iteration 150| Training loss:  3.0064\n",
      "Epoch 7/100 Iteration 160| Training loss:  2.9725\n",
      "Epoch 7/100 Iteration 170| Training loss:  2.9186\n",
      "Epoch 8/100 Iteration 180| Training loss:  2.8598\n",
      "Epoch 8/100 Iteration 190| Training loss:  2.8285\n",
      "Epoch 8/100 Iteration 200| Training loss:  2.7631\n",
      "Epoch 9/100 Iteration 210| Training loss:  2.7621\n",
      "Epoch 9/100 Iteration 220| Training loss:  2.6996\n",
      "Epoch 10/100 Iteration 230| Training loss:  2.6674\n",
      "Epoch 10/100 Iteration 240| Training loss:  2.6390\n",
      "Epoch 10/100 Iteration 250| Training loss:  2.5890\n",
      "Epoch 11/100 Iteration 260| Training loss:  2.6022\n",
      "Epoch 11/100 Iteration 270| Training loss:  2.5468\n",
      "Epoch 12/100 Iteration 280| Training loss:  2.5288\n",
      "Epoch 12/100 Iteration 290| Training loss:  2.5326\n",
      "Epoch 12/100 Iteration 300| Training loss:  2.4761\n",
      "Epoch 13/100 Iteration 310| Training loss:  2.5096\n",
      "Epoch 13/100 Iteration 320| Training loss:  2.4516\n",
      "Epoch 14/100 Iteration 330| Training loss:  2.4367\n",
      "Epoch 14/100 Iteration 340| Training loss:  2.4496\n",
      "Epoch 14/100 Iteration 350| Training loss:  2.4012\n",
      "Epoch 15/100 Iteration 360| Training loss:  2.4309\n",
      "Epoch 15/100 Iteration 370| Training loss:  2.3841\n",
      "Epoch 16/100 Iteration 380| Training loss:  2.3793\n",
      "Epoch 16/100 Iteration 390| Training loss:  2.3991\n",
      "Epoch 16/100 Iteration 400| Training loss:  2.3429\n",
      "Epoch 17/100 Iteration 410| Training loss:  2.3880\n",
      "Epoch 17/100 Iteration 420| Training loss:  2.3451\n",
      "Epoch 18/100 Iteration 430| Training loss:  2.3361\n",
      "Epoch 18/100 Iteration 440| Training loss:  2.3615\n",
      "Epoch 18/100 Iteration 450| Training loss:  2.2992\n",
      "Epoch 19/100 Iteration 460| Training loss:  2.3413\n",
      "Epoch 19/100 Iteration 470| Training loss:  2.3080\n",
      "Epoch 20/100 Iteration 480| Training loss:  2.3028\n",
      "Epoch 20/100 Iteration 490| Training loss:  2.3151\n",
      "Epoch 20/100 Iteration 500| Training loss:  2.2643\n",
      "Epoch 21/100 Iteration 510| Training loss:  2.3208\n",
      "Epoch 21/100 Iteration 520| Training loss:  2.2659\n",
      "Epoch 22/100 Iteration 530| Training loss:  2.2697\n",
      "Epoch 22/100 Iteration 540| Training loss:  2.3119\n",
      "Epoch 22/100 Iteration 550| Training loss:  2.2301\n",
      "Epoch 23/100 Iteration 560| Training loss:  2.2881\n",
      "Epoch 23/100 Iteration 570| Training loss:  2.2384\n",
      "Epoch 24/100 Iteration 580| Training loss:  2.2490\n",
      "Epoch 24/100 Iteration 590| Training loss:  2.2690\n",
      "Epoch 24/100 Iteration 600| Training loss:  2.2053\n",
      "Epoch 25/100 Iteration 610| Training loss:  2.2586\n",
      "Epoch 25/100 Iteration 620| Training loss:  2.2134\n",
      "Epoch 26/100 Iteration 630| Training loss:  2.2104\n",
      "Epoch 26/100 Iteration 640| Training loss:  2.2485\n",
      "Epoch 26/100 Iteration 650| Training loss:  2.1816\n",
      "Epoch 27/100 Iteration 660| Training loss:  2.2398\n",
      "Epoch 27/100 Iteration 670| Training loss:  2.1892\n",
      "Epoch 28/100 Iteration 680| Training loss:  2.2045\n",
      "Epoch 28/100 Iteration 690| Training loss:  2.2260\n",
      "Epoch 28/100 Iteration 700| Training loss:  2.1639\n",
      "Epoch 29/100 Iteration 710| Training loss:  2.2243\n",
      "Epoch 29/100 Iteration 720| Training loss:  2.1841\n",
      "Epoch 30/100 Iteration 730| Training loss:  2.1739\n",
      "Epoch 30/100 Iteration 740| Training loss:  2.2171\n",
      "Epoch 30/100 Iteration 750| Training loss:  2.1308\n",
      "Epoch 31/100 Iteration 760| Training loss:  2.2085\n",
      "Epoch 31/100 Iteration 770| Training loss:  2.1698\n",
      "Epoch 32/100 Iteration 780| Training loss:  2.1701\n",
      "Epoch 32/100 Iteration 790| Training loss:  2.1934\n",
      "Epoch 32/100 Iteration 800| Training loss:  2.1282\n",
      "Epoch 33/100 Iteration 810| Training loss:  2.1818\n",
      "Epoch 33/100 Iteration 820| Training loss:  2.1471\n",
      "Epoch 34/100 Iteration 830| Training loss:  2.1589\n",
      "Epoch 34/100 Iteration 840| Training loss:  2.1730\n",
      "Epoch 34/100 Iteration 850| Training loss:  2.1061\n",
      "Epoch 35/100 Iteration 860| Training loss:  2.1652\n",
      "Epoch 35/100 Iteration 870| Training loss:  2.1315\n",
      "Epoch 36/100 Iteration 880| Training loss:  2.1319\n",
      "Epoch 36/100 Iteration 890| Training loss:  2.1631\n",
      "Epoch 36/100 Iteration 900| Training loss:  2.0897\n",
      "Epoch 37/100 Iteration 910| Training loss:  2.1503\n",
      "Epoch 37/100 Iteration 920| Training loss:  2.1185\n",
      "Epoch 38/100 Iteration 930| Training loss:  2.1266\n",
      "Epoch 38/100 Iteration 940| Training loss:  2.1493\n",
      "Epoch 38/100 Iteration 950| Training loss:  2.0867\n",
      "Epoch 39/100 Iteration 960| Training loss:  2.1412\n",
      "Epoch 39/100 Iteration 970| Training loss:  2.1086\n",
      "Epoch 40/100 Iteration 980| Training loss:  2.1227\n",
      "Epoch 40/100 Iteration 990| Training loss:  2.1340\n",
      "Epoch 40/100 Iteration 1000| Training loss:  2.0662\n",
      "Epoch 41/100 Iteration 1010| Training loss:  2.1245\n",
      "Epoch 41/100 Iteration 1020| Training loss:  2.0883\n",
      "Epoch 42/100 Iteration 1030| Training loss:  2.0968\n",
      "Epoch 42/100 Iteration 1040| Training loss:  2.1190\n",
      "Epoch 42/100 Iteration 1050| Training loss:  2.0584\n",
      "Epoch 43/100 Iteration 1060| Training loss:  2.1029\n",
      "Epoch 43/100 Iteration 1070| Training loss:  2.0789\n",
      "Epoch 44/100 Iteration 1080| Training loss:  2.0841\n",
      "Epoch 44/100 Iteration 1090| Training loss:  2.1207\n",
      "Epoch 44/100 Iteration 1100| Training loss:  2.0330\n",
      "Epoch 45/100 Iteration 1110| Training loss:  2.0851\n",
      "Epoch 45/100 Iteration 1120| Training loss:  2.0610\n",
      "Epoch 46/100 Iteration 1130| Training loss:  2.0736\n",
      "Epoch 46/100 Iteration 1140| Training loss:  2.0935\n",
      "Epoch 46/100 Iteration 1150| Training loss:  2.0305\n",
      "Epoch 47/100 Iteration 1160| Training loss:  2.0748\n",
      "Epoch 47/100 Iteration 1170| Training loss:  2.0435\n",
      "Epoch 48/100 Iteration 1180| Training loss:  2.0540\n",
      "Epoch 48/100 Iteration 1190| Training loss:  2.0887\n",
      "Epoch 48/100 Iteration 1200| Training loss:  2.0196\n",
      "Epoch 49/100 Iteration 1210| Training loss:  2.0647\n",
      "Epoch 49/100 Iteration 1220| Training loss:  2.0310\n",
      "Epoch 50/100 Iteration 1230| Training loss:  2.0449\n",
      "Epoch 50/100 Iteration 1240| Training loss:  2.0823\n",
      "Epoch 50/100 Iteration 1250| Training loss:  2.0089\n",
      "Epoch 51/100 Iteration 1260| Training loss:  2.0520\n",
      "Epoch 51/100 Iteration 1270| Training loss:  2.0323\n",
      "Epoch 52/100 Iteration 1280| Training loss:  2.0441\n",
      "Epoch 52/100 Iteration 1290| Training loss:  2.0582\n",
      "Epoch 52/100 Iteration 1300| Training loss:  1.9926\n",
      "Epoch 53/100 Iteration 1310| Training loss:  2.0432\n",
      "Epoch 53/100 Iteration 1320| Training loss:  2.0104\n",
      "Epoch 54/100 Iteration 1330| Training loss:  2.0359\n",
      "Epoch 54/100 Iteration 1340| Training loss:  2.0570\n",
      "Epoch 54/100 Iteration 1350| Training loss:  1.9848\n",
      "Epoch 55/100 Iteration 1360| Training loss:  2.0254\n",
      "Epoch 55/100 Iteration 1370| Training loss:  2.0084\n",
      "Epoch 56/100 Iteration 1380| Training loss:  2.0124\n",
      "Epoch 56/100 Iteration 1390| Training loss:  2.0431\n",
      "Epoch 56/100 Iteration 1400| Training loss:  1.9743\n",
      "Epoch 57/100 Iteration 1410| Training loss:  2.0130\n",
      "Epoch 57/100 Iteration 1420| Training loss:  2.0028\n",
      "Epoch 58/100 Iteration 1430| Training loss:  2.0046\n",
      "Epoch 58/100 Iteration 1440| Training loss:  2.0268\n",
      "Epoch 58/100 Iteration 1450| Training loss:  1.9643\n",
      "Epoch 59/100 Iteration 1460| Training loss:  2.0161\n",
      "Epoch 59/100 Iteration 1470| Training loss:  1.9922\n",
      "Epoch 60/100 Iteration 1480| Training loss:  1.9964\n",
      "Epoch 60/100 Iteration 1490| Training loss:  2.0213\n",
      "Epoch 60/100 Iteration 1500| Training loss:  1.9518\n",
      "Epoch 61/100 Iteration 1510| Training loss:  1.9973\n",
      "Epoch 61/100 Iteration 1520| Training loss:  1.9766\n",
      "Epoch 62/100 Iteration 1530| Training loss:  1.9799\n",
      "Epoch 62/100 Iteration 1540| Training loss:  2.0071\n",
      "Epoch 62/100 Iteration 1550| Training loss:  1.9473\n",
      "Epoch 63/100 Iteration 1560| Training loss:  1.9908\n",
      "Epoch 63/100 Iteration 1570| Training loss:  1.9684\n",
      "Epoch 64/100 Iteration 1580| Training loss:  1.9724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 Iteration 1590| Training loss:  1.9991\n",
      "Epoch 64/100 Iteration 1600| Training loss:  1.9355\n",
      "Epoch 65/100 Iteration 1610| Training loss:  1.9746\n",
      "Epoch 65/100 Iteration 1620| Training loss:  1.9674\n",
      "Epoch 66/100 Iteration 1630| Training loss:  1.9594\n",
      "Epoch 66/100 Iteration 1640| Training loss:  1.9922\n",
      "Epoch 66/100 Iteration 1650| Training loss:  1.9276\n",
      "Epoch 67/100 Iteration 1660| Training loss:  1.9815\n",
      "Epoch 67/100 Iteration 1670| Training loss:  1.9496\n",
      "Epoch 68/100 Iteration 1680| Training loss:  1.9727\n",
      "Epoch 68/100 Iteration 1690| Training loss:  1.9804\n",
      "Epoch 68/100 Iteration 1700| Training loss:  1.9223\n",
      "Epoch 69/100 Iteration 1710| Training loss:  1.9642\n",
      "Epoch 69/100 Iteration 1720| Training loss:  1.9473\n",
      "Epoch 70/100 Iteration 1730| Training loss:  1.9635\n",
      "Epoch 70/100 Iteration 1740| Training loss:  1.9821\n",
      "Epoch 70/100 Iteration 1750| Training loss:  1.9173\n",
      "Epoch 71/100 Iteration 1760| Training loss:  1.9541\n",
      "Epoch 71/100 Iteration 1770| Training loss:  1.9404\n",
      "Epoch 72/100 Iteration 1780| Training loss:  1.9489\n",
      "Epoch 72/100 Iteration 1790| Training loss:  1.9708\n",
      "Epoch 72/100 Iteration 1800| Training loss:  1.9014\n",
      "Epoch 73/100 Iteration 1810| Training loss:  1.9401\n",
      "Epoch 73/100 Iteration 1820| Training loss:  1.9318\n",
      "Epoch 74/100 Iteration 1830| Training loss:  1.9467\n",
      "Epoch 74/100 Iteration 1840| Training loss:  1.9538\n",
      "Epoch 74/100 Iteration 1850| Training loss:  1.9009\n",
      "Epoch 75/100 Iteration 1860| Training loss:  1.9427\n",
      "Epoch 75/100 Iteration 1870| Training loss:  1.9271\n",
      "Epoch 76/100 Iteration 1880| Training loss:  1.9328\n",
      "Epoch 76/100 Iteration 1890| Training loss:  1.9458\n",
      "Epoch 76/100 Iteration 1900| Training loss:  1.8923\n",
      "Epoch 77/100 Iteration 1910| Training loss:  1.9302\n",
      "Epoch 77/100 Iteration 1920| Training loss:  1.9090\n",
      "Epoch 78/100 Iteration 1930| Training loss:  1.9317\n",
      "Epoch 78/100 Iteration 1940| Training loss:  1.9398\n",
      "Epoch 78/100 Iteration 1950| Training loss:  1.8809\n",
      "Epoch 79/100 Iteration 1960| Training loss:  1.9272\n",
      "Epoch 79/100 Iteration 1970| Training loss:  1.9148\n",
      "Epoch 80/100 Iteration 1980| Training loss:  1.9138\n",
      "Epoch 80/100 Iteration 1990| Training loss:  1.9431\n",
      "Epoch 80/100 Iteration 2000| Training loss:  1.8834\n",
      "Epoch 81/100 Iteration 2010| Training loss:  1.9133\n",
      "Epoch 81/100 Iteration 2020| Training loss:  1.8948\n",
      "Epoch 82/100 Iteration 2030| Training loss:  1.8998\n",
      "Epoch 82/100 Iteration 2040| Training loss:  1.9357\n",
      "Epoch 82/100 Iteration 2050| Training loss:  1.8708\n",
      "Epoch 83/100 Iteration 2060| Training loss:  1.9095\n",
      "Epoch 83/100 Iteration 2070| Training loss:  1.8981\n",
      "Epoch 84/100 Iteration 2080| Training loss:  1.9011\n",
      "Epoch 84/100 Iteration 2090| Training loss:  1.9212\n",
      "Epoch 84/100 Iteration 2100| Training loss:  1.8632\n",
      "Epoch 85/100 Iteration 2110| Training loss:  1.9054\n",
      "Epoch 85/100 Iteration 2120| Training loss:  1.8869\n",
      "Epoch 86/100 Iteration 2130| Training loss:  1.9102\n",
      "Epoch 86/100 Iteration 2140| Training loss:  1.9205\n",
      "Epoch 86/100 Iteration 2150| Training loss:  1.8731\n",
      "Epoch 87/100 Iteration 2160| Training loss:  1.9006\n",
      "Epoch 87/100 Iteration 2170| Training loss:  1.8836\n",
      "Epoch 88/100 Iteration 2180| Training loss:  1.8960\n",
      "Epoch 88/100 Iteration 2190| Training loss:  1.9139\n",
      "Epoch 88/100 Iteration 2200| Training loss:  1.8483\n",
      "Epoch 89/100 Iteration 2210| Training loss:  1.9016\n",
      "Epoch 89/100 Iteration 2220| Training loss:  1.8756\n",
      "Epoch 90/100 Iteration 2230| Training loss:  1.8861\n",
      "Epoch 90/100 Iteration 2240| Training loss:  1.9023\n",
      "Epoch 90/100 Iteration 2250| Training loss:  1.8605\n",
      "Epoch 91/100 Iteration 2260| Training loss:  1.8955\n",
      "Epoch 91/100 Iteration 2270| Training loss:  1.8691\n",
      "Epoch 92/100 Iteration 2280| Training loss:  1.8823\n",
      "Epoch 92/100 Iteration 2290| Training loss:  1.9009\n",
      "Epoch 92/100 Iteration 2300| Training loss:  1.8472\n",
      "Epoch 93/100 Iteration 2310| Training loss:  1.8846\n",
      "Epoch 93/100 Iteration 2320| Training loss:  1.8620\n",
      "Epoch 94/100 Iteration 2330| Training loss:  1.8689\n",
      "Epoch 94/100 Iteration 2340| Training loss:  1.8899\n",
      "Epoch 94/100 Iteration 2350| Training loss:  1.8424\n",
      "Epoch 95/100 Iteration 2360| Training loss:  1.8752\n",
      "Epoch 95/100 Iteration 2370| Training loss:  1.8626\n",
      "Epoch 96/100 Iteration 2380| Training loss:  1.8696\n",
      "Epoch 96/100 Iteration 2390| Training loss:  1.9009\n",
      "Epoch 96/100 Iteration 2400| Training loss:  1.8338\n",
      "Epoch 97/100 Iteration 2410| Training loss:  1.8768\n",
      "Epoch 97/100 Iteration 2420| Training loss:  1.8668\n",
      "Epoch 98/100 Iteration 2430| Training loss:  1.8584\n",
      "Epoch 98/100 Iteration 2440| Training loss:  1.8919\n",
      "Epoch 98/100 Iteration 2450| Training loss:  1.8314\n",
      "Epoch 99/100 Iteration 2460| Training loss:  1.8514\n",
      "Epoch 99/100 Iteration 2470| Training loss:  1.8481\n",
      "Epoch 100/100 Iteration 2480| Training loss:  1.8563\n",
      "Epoch 100/100 Iteration 2490| Training loss:  1.8735\n",
      "Epoch 100/100 Iteration 2500| Training loss:  1.8174\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100 \n",
    "train_x, train_y = reshape_data(text_ints, \n",
    "                                batch_size, \n",
    "                                num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, \n",
    "          num_epochs=100,\n",
    "          ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <<lstm_outputs   >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(1, 65), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./model-100/language_modeling.ckpt\n",
      "The shist at his wish these shat in the pelains andere and muscard, a dowht, a thinkent as of is it mant, wothere this thay, what, ard haul his whese: whene he heard, on the toull what of thoues\n",
      "To may in ore inta chaust hemeete, that whel he hould\n",
      "Te thit tos so toust of thay in may,\n",
      "Thoued and a deene\n",
      "To me thit thas simpald the sallent in as in the,\n",
      "Whin words in that sine the preserind of and miner and\n",
      "Andowne thinge tis allath that wat he well would,\n",
      "To shaule so the ploshe wist my some,\n",
      "Tare in\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "\n",
    "print(rnn.sample(ckpt_dir='./model-100/', \n",
    "                 output_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
