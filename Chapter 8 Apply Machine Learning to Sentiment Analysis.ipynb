{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapter 8 Applying Machine Learning to Sentiment Analysis\n",
    "\n",
    "#How to classify documents based on their polarity:\n",
    "#the attitude of the writer\n",
    "\n",
    "#In particular, we are going to work with a dataset of 50000 movie reviews from IMDb\n",
    "# and build a predictor that can distinguish between positive and negative reviews\n",
    "\n",
    "# We will cover:\n",
    "# 1. Cleaning and preparing text data\n",
    "# 2. Building feature vectors from text documents\n",
    "# 3. Training a machine learning model to classify positive and negative movie reviews\n",
    "# 4. Working with large text datasets using out-of-core learning\n",
    "# 5. Inferring topics from document collections for categorization\n",
    "\n",
    "\n",
    "#Preparing the IMDb movie review data for text processing\n",
    "\n",
    "#Sentiment analysis, sometimes also called opinion mining\n",
    "# analyzing the polarity of documents\n",
    "# classification of documents based on the expressed opinions or emotions of the authors\n",
    "\n",
    "# We will work on a large dataset of movie reviews from the IMDb\n",
    "# consist of 50,000 polar movie reviews that are labeled as either positive or negative\n",
    "# Positive: a movie was rated with more than six stars\n",
    "# Negative: a movie was rated with fewer than five stars\n",
    "\n",
    "# We will:\n",
    "# 1. Download the dataset\n",
    "# 2. Preprocess it into a useable format for machine learning tools\n",
    "# 3. Extract meaningful information from a subset of these movie reviews to build a machine learning model\n",
    "# Goal: predict whether a certain reviewer liked or disliked a movie\n",
    "\n",
    "#Preparing the IMDb movie review data for text processing\n",
    "\n",
    "#Obtaining the IMDb moview review dataset\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "target = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "#code to download the dataset\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = progress_size / (1024.**2 * duration)\n",
    "    percent = count * block_size * 100. / total_size\n",
    "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed\" %\n",
    "                     (percent, progress_size / (1024.**2), speed, duration))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "    if (sys.version_info < (3,0)):\n",
    "        import urllib\n",
    "        urllib.urlretrieve(source,target,reporthook)\n",
    "    else:\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(source,target,reporthook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to unzip the dataset\n",
    "# directly unpack the Gzip-compressed tarball archive direcly in Python\n",
    "\n",
    "if not os.path.isdir('aclImdb'):\n",
    "    with tarfile.open(target, 'r:gz') as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:35\n"
     ]
    }
   ],
   "source": [
    "#read the movies intoa pandas DataFrame object\n",
    "#take upto 10 miinutes\n",
    "#To visualize the progress and estimated time until completion\n",
    "# Python Progress Indicator (PyPrind): pip install pyprind\n",
    "\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#change the `basepath` to the directory of the unzipped movie dataset\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos':1, 'neg':0}\n",
    "#initialized a new progress bar object pbar\n",
    "#with 50,000 iterations, which is the # of docs we will read in\n",
    "pbar = pyprind.ProgBar(50000) \n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test','train'): #iterate over the train and test subdirectories\n",
    "    for l in ('pos','neg'): #read the individual text files from the pos and neg subdirectories\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file),\n",
    "                      'r', encoding = 'utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "                #append text files to the df pandas DataFrame\n",
    "            df = df.append([[txt,labels[l]]],ignore_index=True)\n",
    "            pbar.update()\n",
    "\n",
    "df.columns = ['review','sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the class labels in the assembled dataset are sorted\n",
    "# need shuffle the DataFrame using the permutation function from the np.random submodule\n",
    "# useful to split the dataset into training and test sets\n",
    "\n",
    "# stream the data from our local drive directly\n",
    "# store the assembled and shuffled movie review dataset as CSV file\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# Shuffle\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the assembled data as CSV file\n",
    "\n",
    "df.to_csv('movie_data.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tearful movie about a sister and her batt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's too kind to call this a \"fictionalized\" a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truly bad and easily the worst episode I have ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  This tearful movie about a sister and her batt...          1\n",
       "1  It's too kind to call this a \"fictionalized\" a...          0\n",
       "2  Truly bad and easily the worst episode I have ...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv',encoding = 'utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing the bag-of-words model\n",
    "# represent text as numerical feature vectors\n",
    "# Idea:\n",
    "# 1. Create a vocabulary unique tokens\n",
    "# 2. Construct a feature vector from each documents \n",
    "#    that contains the frequency of the words in each documents\n",
    "\n",
    "# Since the unique words in each documents represent only a small subset\n",
    "# of all the words in the bag-of-words vocabulary\n",
    "# the feature vectors will mostly consist of zeros ==> Sparse\n",
    "\n",
    "# Transforming documents into feature vectors\n",
    "\n",
    "# By calling the fit_transform method on CountVectorizer\n",
    "# we just constructed the vocabulary of the bag-of-words model \n",
    "# and transformed the following 3 sentences into sparse feature vectors\n",
    "\n",
    "# 1. The sun is shining\n",
    "# 2. The weather is sweet\n",
    "# 3. The sun is shining, the weather is sweet, and one and one is two\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "# The vocabularty is stored in a Python dictionary \n",
    "# that maps the unique words to integer indices (not count) \n",
    "\n",
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Next let us print the feature vectors that we just created:\n",
    "\n",
    "# Each index position in the feature vectors shown here corresponds to \n",
    "# the integer values that are stored as dictionary items in the CountVectorizer vocabulary. \n",
    "# For example, the rst feature at index position 0 resembles the count of the word and, \n",
    "# which only occurs in the last document, \n",
    "# and the word is at index position 1 (the 2nd feature in the document vectors) occurs in all three sentences. \n",
    "# Those values in the feature vectors are also called \n",
    "# the raw term frequencies: tf (t,d)\n",
    "# —the number of times a term t occurs in a document d.\n",
    "\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing word relevancy via term frequency-inverse doument frequency\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. \n",
    "\n",
    "Term frequency-inverse document frequency (tf-idf):\n",
    "- Downweight those frequently occurring words in the feature vectors. \n",
    "\n",
    "The tf-idf can be defined as the product of the term frequency and the inverse document frequency:\n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
    "\n",
    "tf(t, d) is the term frequency\n",
    "inverse document frequency *idf(t, d)* can be calculated as:\n",
    "\n",
    "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
    "\n",
    "where $n_d$ is the total number of documents, and *df(d, t)* is the number of documents *d* that contain the term *t*. \n",
    "\n",
    "Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight.\n",
    "\n",
    "Scikit-learn implements yet another transformer, the `TfidfTransformer`, that takes the raw term frequencies from `CountVectorizer` as input and transforms them into tf-idfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf = True,\n",
    "                         norm = 'l2',\n",
    "                         smooth_idf = True)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous subsection, the word 'is' had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, we see that the word 'is' is now associated with a relatively small tf-idf (0.45) in document 3 since it is also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information.\n",
    "\n",
    "However, `TfidfTransformer` calculates the tf-idfs slightly differently compared to the standard textbook equations that we defined earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are:\n",
    "\n",
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "\n",
    "The tf-idf equation that was implemented in scikit-learn is as follows:\n",
    "\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the `TfidfTransformer` normalizes the tf-idfs directly.\n",
    "\n",
    "By default (`norm='l2'`), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm:\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "\n",
    "To make sure that we understand how TfidfTransformer works, let us walk\n",
    "through an example and calculate the tf-idf of the word is in the 3rd document.\n",
    "\n",
    "The word is has a term frequency of 3 (tf = 3) in document 3, and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n",
    "\n",
    "$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
    "\n",
    "Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n",
    "\n",
    "$$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs +1)/(3+1))\n",
    "tfidf_is = tf_is * (idf_is +1)\n",
    "\n",
    "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeated these calculations for all terms in the 3rd document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. However, we notice that the values in this feature vector are different from the values that we obtained from the TfidfTransformer that we used previously. The  nal step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:\n",
    "\n",
    "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n",
    "\n",
    "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
    "\n",
    "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$\n",
    "\n",
    "As we can see, the results match the results returned by scikit-learn's `TfidfTransformer` (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.39, 3.  , 3.39, 1.29, 1.29, 1.29, 2.  , 1.69, 1.29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replicate the 'TfidfTransformer''s result\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm = None, smooth_idf = True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wonderful journey from life to death.<br /><br />'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean text data\n",
    "\n",
    "# Before building our bag-of-words model,\n",
    "# we need to clean the text data by stripping it of all unwanted characters\n",
    "\n",
    "# Display the last 50 characters from the first document in the reshuffled movie review dataset\n",
    "# the text contains HTML markup and punctuation and other non-letter characters\n",
    "\n",
    "df.loc[0,'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though punctuation marks can represent useful, additional information in NLP contexts\n",
    "# for simplicity, we will remove all punctuation marks \n",
    "# except for emoticon characters such as :) as those are useful for sentiment analysis\n",
    "# we will use Python's regular expression (regex) library, `re`\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    \n",
    "    #remove all pf the HTML markup from the movie reviews\n",
    "    text = re.sub('<[^>]*>','',text)\n",
    "    \n",
    "    #find emoticons, temporarily stored as emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    \n",
    "    #remove all non-word characters from the text via the regex [\\W]+\n",
    "    #and convert the text into lowercase characters\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) + \n",
    "            ' '.join(emoticons).replace('-','')) # add the temporarily stored emoticons to the end of the processed document string\n",
    "    #also, :-) is also converted to :) for consistency (removing the '-')\n",
    "\n",
    "    # although the addition of the emotiocon characters to the end of the cleaned document string\n",
    "    # may not look like the most elegant approach,\n",
    "    # the order of the words does not matter in our bag-of-words model \n",
    "    # if our vocabulary consists of only one-word tokens\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wonderful journey from life to death '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the preprocessor work correctly\n",
    "preprocessor(df.loc[0,'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the preprocessor work correctly\n",
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the preprocessor function to all the movie reviews in our DataFrame\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing documents into tokens\n",
    "\n",
    "#split the text corpora into individual elements\n",
    "\n",
    "#Natural Language Toolkit (NLTK)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    # tokenize the document by splitting the clenaned documents\n",
    "    # at its whitespace characters\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    #word stemming + tokenization\n",
    "    # transform a word into its root form\n",
    "    # Porter stemmer algorithm\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')\n",
    "\n",
    "# stemming may create non-real words.\n",
    "# Lemmatization aims to obtain the canonical (grammatically correct)\n",
    "# forms of individual words - the so-called lemmas\n",
    "# but computationally more difficult and expensive compared to stemming\n",
    "# and in practice, it has been observed that stemming and lemmatization \n",
    "# have little impact on the performance of text classfication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/Rex/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop-word removal\n",
    "# Stop-words: words that are extremely common in all sorts of texts and probably bear no (or little) useful information\n",
    "# e.g. is, and, has, and like\n",
    "# Removing stop-words can be useful if we are working with raw or normalized term frequencies \n",
    "# rather than tf-idfs whih are already downweighting frequently occuring words\n",
    "\n",
    "# use the set of 127 English stop-words that is available from the NLTK library\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#load and apply the English stop-word set as follows:\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a logistic regression model for document classification\n",
    "# strip HTML and punctuation to speed up the GridSearch later:\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# `GridSearchCV object to find the optimal set of parameters \n",
    "# for our logistic regreesion model\n",
    "# using 5-fold stratified cross-validation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TfidfVectorizer = CountVectorizer + TfidfTransformer\n",
    "tfidf = TfidfVectorizer(strip_accents = None,\n",
    "                        lowercase = False,\n",
    "                        preprocessor = None)\n",
    "\n",
    "# 2 parameter dictionaries: \n",
    "#   1) TfidVectorizer with its default setting\n",
    "#   2) set use_idf = False, smooth_idf = False, and norm = None\n",
    "#    to train a model based on raw term frequencies\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'], # regularization\n",
    "               'clf__C': [1.0, 10.0, 100.0]}, # regularization strength\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'], # regularization\n",
    "               'clf__C': [1.0, 10.0, 100.0]} # regularization strength\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                            scoring = 'accuracy',\n",
    "                            cv = 5,\n",
    "                            verbose = 1,\n",
    "                            n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note about `n_jobs`**\n",
    "\n",
    "Please note that it is highly recommended to use `n_jobs=-1` (instead of `n_jobs=1`) in the previous code example to utilize all available cores on your machine and speed up the grid search. However, some Windows users reported issues when running the previous code with the `n_jobs=-1` setting related to pickling the tokenizer and tokenizer_porter functions for multiprocessing on Windows. Another workaround would be to replace those two functions, `[tokenizer, tokenizer_porter]`, with `[str.split]`. However, note that the replacement by the simple `str.split` would not support stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note about the running time**\n",
    "\n",
    "Executing the following code cell **may take up to 30-60 min** depending on your machine, since based on the parameter grid we defined, there are 2*2*2*3*5 + 2*2*2*3*5 = 240 models to fit.\n",
    "\n",
    "If you do not wish to wait so long, you could reduce the size of the dataset by decreasing the number of training samples, for example, as follows:\n",
    "\n",
    "    X_train = df.loc[:2500, 'review'].values\n",
    "    y_train = df.loc[:2500, 'sentiment'].values\n",
    "    \n",
    "However, note that decreasing the training set size to such a small number will likely result in poorly performing models. Alternatively, you can delete parameters from the grid above to reduce the number of models to fit -- for example, by using the following:\n",
    "\n",
    "    param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "                   'vect__stop_words': [stop, None],\n",
    "                   'vect__tokenizer': [tokenizer],\n",
    "                   'clf__penalty': ['l1', 'l2'],\n",
    "                   'clf__C': [1.0, 10.0]},\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 47.8min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 65.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's...se_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7fab90e59ea0>} \n",
      "CV Accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.901\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Start comment:\n",
    "    \n",
    "Please note that `gs_lr_tfidf.best_score_` is the average k-fold cross-validation score. I.e., if we have a `GridSearchCV` object with 5-fold cross-validation (like the one above), the `best_score_` attribute returns the average score over the 5-folds of the best model. To illustrate this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0.4, 0.6, 0.2, 0.6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#create a simple data set of random integers that shall represent out class labels\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision = 6)\n",
    "y = [np.random.randint(3) for i in range(25)]\n",
    "X = (y+np.random.randn(25)).reshape(-1,1)\n",
    "\n",
    "#feed the indices of 5 cross-validation folds `cv5_idx) to the scorer\n",
    "#returning 5 accuracy scores\n",
    "cv5_idx = list(StratifiedKFold(n_splits = 5, shuffle = False, random_state=0).split(X,y))\n",
    "\n",
    "cross_val_score(LogisticRegression(random_state=123),X,y,cv=cv5_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing the code above, we created a simple data set of random integers that shall represent our class labels. Next, we fed the indices of 5 cross-validation folds (`cv5_idx`) to the `cross_val_score` scorer, which returned 5 accuracy scores -- these are the 5 accuracy values for the 5 test folds.  \n",
    "\n",
    "Next, let us use the `GridSearchCV` object and feed it the same 5 cross-validation sets (via the pre-generated `cv3_idx` indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=0.6, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=0.4, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=0.6, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=0.2, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=0.6, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(),{},cv=cv5_idx,verbose=3).fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the scores for the 5 folds are exactly the same as the ones from `cross_val_score` earlier.\n",
    "\n",
    "Now, the best_score_ attribute of the `GridSearchCV` object, which becomes available after `fit`ting, returns the average accuracy score of the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the result above is consistent with the average score computed the `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(), X, y, cv=cv5_idx).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End comment.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with Bigger data - online algorithms and out-of-core learning\n",
    "\n",
    "# This cell is added for convenience so that the notebook can\n",
    "# be executed starting here, without executing prior code in\n",
    "# notebook\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "if not os.path.isfile('movie_data.csv'):\n",
    "    if not os.path.isfile('movie_data.csv.gz'):\n",
    "        print('Please place a copy of the movie_data.csv.gz'\n",
    "              'in this directory. You can obtain it by'\n",
    "              'a) executing the code in the beginning of this'\n",
    "              'notebook or b) by downloading it from GitHub:'\n",
    "              'https://github.com/rasbt/python-machine-learning-'\n",
    "              'book-2nd-edition/blob/master/code/ch08/movie_data.csv.gz')\n",
    "    else:\n",
    "        in_f = gzip.open('movie_data.csv.gz','rb')\n",
    "        out_f = open('movie_data.csv','wb')\n",
    "        out_f.write(in_f.read())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-core learning:\n",
    "# work with large datasets by fitting the classifier incrementally\n",
    "# on smaller bathces of the datasets\n",
    "\n",
    "# Stochastic gradient descent\n",
    "# optimization algorithm that updates the model's weight using one sample at a time\n",
    "\n",
    "# make use of the `partial_fit` function of the `SGDClassifier` in scikit-learn\n",
    "# to steam the documents directly from our local drive\n",
    "# and train a logistic regression model using small mini-batches of documents\n",
    "\n",
    "import numpy as np \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# The `stop` is defined as earlier in this chapter\n",
    "# Added it here for convenience, so that this section\n",
    "# can be run as standalone without executing prior code\n",
    "# in the directory\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "# clearn the unprocessed text data from the movie_data.csv file\n",
    "# separate it into word tokens while removing stop words\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    # reads in and returns one documents at a time\n",
    "    with open(path,'r',encoding='utf-8') as csv:\n",
    "        next(csv) #skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])           \n",
    "            yield text, label            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This tearful movie about a sister and her battle to save as many souls as she can is very moving. The film does well in picking up the characters and showing how Sister Helen deals with each.<br /><br />A wonderful journey from life to death.<br /><br />',\n",
       " 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to verify that our stream_docs function works correctly\n",
    "# read in the first document from the `movie_data.csv`\n",
    "# return a tuple consisting of the review text and the corresponding class label\n",
    "\n",
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream,size):\n",
    "\n",
    "# take a document stream from the stream_docs function and\n",
    "# return a particular documents specified by the size parameter\n",
    "    \n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot use CountVectorizer or TfidfVectorizer\n",
    "# CountVectorizer requires holding the complete vocabulary in memory\n",
    "# TfidVectorizer needs to keep all the feature vectors of the training dataset in memeory to calculate the inverse document frequencies\n",
    "\n",
    "# But we can use HashingVectorizer in scikie-learn\n",
    "# data-independent, makes uses of the hashing trick via the 32-bit MurmurHash3 function\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features = 2**21, # large number of features to reduce the change of causing hash collisions\n",
    "                         #but also increase the number of coefficients in our logistic regression model\n",
    "                         preprocessor = None,\n",
    "                         tokenizer = tokenizer)\n",
    "\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "if Version(sklearn_version) < '0.18':\n",
    "    clf = SGDClassifier(loss='log', random_state = 1, n_iter = 1)\n",
    "    #reinitialse a logistic regression classifier by setting the loss parameter to `log`\n",
    "else:\n",
    "    clf = SGDClassifier(loss='log', random_state = 1, max_iter = 1)\n",
    "    \n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    }
   ],
   "source": [
    "import pyprind # estimate the progress of our learning algorithm\n",
    "\n",
    "pbar = pyprind.ProgBar(45) # initial the progress bar object with 45 iterations\n",
    "\n",
    "classes = np.array([0,1])\n",
    "\n",
    "for _ in range(45): #iterate over 45 mini-batches of documents\n",
    "    X_train, y_train = get_minibatch(doc_stream,size =1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size = 5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the last 5,000 docs to update our model\n",
    "\n",
    "clf = clf.partial_fit(X_test,y_test)\n",
    "\n",
    "# word2vec, a modern alternative to the bag-of-words model\n",
    "# unsupervised learning algorithm based on neural networks\n",
    "# attempts to automatically learn the relationship between words\n",
    "# idea: put words that have similar meanings into similar clusters\n",
    "# and via clever vector-spacing, the model can reproduce certain words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tearful movie about a sister and her batt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's too kind to call this a \"fictionalized\" a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truly bad and easily the worst episode I have ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  This tearful movie about a sister and her batt...          1\n",
       "1  It's too kind to call this a \"fictionalized\" a...          0\n",
       "2  Truly bad and easily the worst episode I have ...          0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic modelling:\n",
    "# describe the broad task of assigning topics to unlabelled text documents\n",
    "# categorization, topic-assigning, clustering\n",
    "# unsupervised learning\n",
    "\n",
    "# Decomposing text documents with Latent Dirichlet Allocation (LDA)\n",
    "# NOTE: Latent Dirichlet Allocation (LDA) NOT Linear Discriminant Analysis (LDA)\n",
    "# LDA is a generative probabilistic model that tries to find groups of words\n",
    "# that appear frequently together across different documents\n",
    "# These frequently appearing words, represent our topics, \n",
    "# assuming each documents is a mixture of different words\n",
    "\n",
    "# input: bag-of-words matrix\n",
    "# LDA decomposes the bag-of-words matrix into two new matrices\n",
    "# 1) A document-to-topic matrix, 2) A word-to-topic matrix\n",
    "# if we multiply those 2 matrices together, we would be able to reproduce the input, the bag-of-words\n",
    "# with the lowest possible error\n",
    "# in practice, we are interested in those topics define the number of topics beforehand\n",
    "# the number of topics is a hyperparameter of LDA that has to be specified manually.\n",
    "\n",
    "\n",
    "# Latent Dirchlet Allocation with scikit-learn\n",
    "\n",
    "# Decompose the movie review dataset and categorize it into different topics\n",
    "# restrict the analysis to 10 different topics\n",
    "\n",
    "# First: load the dataset into a pandas DataFrame using the local movie_data.csv\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv',encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the CountVectorizer to create the bag-of-words matrix as input to the LDA\n",
    "# we use scikit-learn's built-in English stop word library via stop_words = 'english'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words = 'english',\n",
    "                        max_df = .1, #maximum document frequency, exclude words that occur too frequently across documents (but set arbitrarily)\n",
    "                        max_features = 5000) #most frequently occuring 5,000 words only (but set arbitrarily), limit the dimensionality\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Rex/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# fit a LatentDirchletAllocation estimator to the bag-of-words matrix\n",
    "# and infer the 10 different topics from the documents\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 10,\n",
    "                                random_state = 123,\n",
    "                                learning_method = 'batch') # do its estimation based on all available training data (the bag-of-words matrix) in one iteration\n",
    "# slower than `online` (analogous to online or mini-batch learning) but more accurate results\n",
    "\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes script awful stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art feel\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex blood gore\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode episodes war tv\n",
      "Topic 9:\n",
      "book version original effects fi\n",
      "Topic 10:\n",
      "action fight guy kids fun\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx +1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort() [:-n_top_words-1:-1]]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on reading the 5 most important words for each topic, we may guess that the LDA identified the following topics:\n",
    "    \n",
    "1. Generally bad movies (not really a topic category)\n",
    "2. Movies about families\n",
    "3. War movies\n",
    "4. Art movies\n",
    "5. Crime movies\n",
    "6. Horror movies\n",
    "7. Comedies\n",
    "8. Movies somehow related to TV shows\n",
    "9. Movies based on books\n",
    "10. Action movies\n",
    "\n",
    "To confirm that the categories make sense based on the reviews, let's plot 5 movies from the horror movie category (category 6 at index position 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n",
      "\n",
      "Horror movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:,5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx +1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
