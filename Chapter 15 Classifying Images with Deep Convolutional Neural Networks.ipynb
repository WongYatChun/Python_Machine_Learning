{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Classifying Images with Deep Convolutional Neural Networks\n",
    "\n",
    "\n",
    "- Understanding convolutional operations in one and two dimensions\n",
    "- Learning about the building blocks of CNN architectures\n",
    "- Implementing deep convolutional neural networks in TensorFlow\n",
    "\n",
    "Key to the performance of any machine learning algorithm\n",
    "- extractung salient features\n",
    "    - Traditional: rely on input features that may come from a domain expert, or are based on computational feature extraction techniques\n",
    "    - Neural networks: automatically learn the features from new raw data that are most useful for a particular task.\n",
    "        - a feature extraction engine: the early layers extract low-level features\n",
    "        - Multilayer neural networks: feature hierachy\n",
    "            - combining the low-level features in a layer-wise fashion to form high-level features\n",
    "            \n",
    "Local receptive field: local patch of pixels\n",
    "\n",
    "### CNN will usually perform very well for image-related tasks, because:\n",
    "1. Sparse-connectivity: a single element in the feature map is connected to only a small patch of pixels\n",
    "2. Parameter-sharing: The same weights are used for different patches of the input image\n",
    "\n",
    "As a direct consequence, the number of weights(parameters) in the network decreases dramatically, and we see an improvement in the ability to capture salient features.\n",
    "\n",
    "Intuitively, nearby pixels are probably more relevant to each other than pixels that are far away each other.\n",
    "\n",
    "### Typically, CNNs are composed of    \n",
    "\n",
    "1. several **Convoluntional(conv)** layers\n",
    "    - with learnable parameters, e.g. weights or bias units\n",
    "\n",
    "\n",
    "2. several subsampling (also known as **Pooling(P)**) layers\n",
    "    - no any learnable parameters, no weights or bias units\n",
    "\n",
    "\n",
    "3. **Fully Connected (PC)** layers\n",
    "    - with learnable parameters, e.g. weights or bias units\n",
    "    - essentially a multilayer perceptron\n",
    "    - every input unit i is connected to every output unit j with weight $w_{ij}$\n",
    "    \n",
    "\n",
    "### Three mode of padding\n",
    "1. Full: the padding parameter $p$ is set to $p = m - 1$ where $m$ is the number of element of the filter\n",
    "    - increase the dimensions of the output\n",
    "    - rarely used in CNN\n",
    "    - usually used in signal processing\n",
    "    \n",
    "2. Same: the padding parameter $p$ is computed according to the filter size, along with the requirement that the input size and the output size are the same\n",
    "    - have the size of the output the same as the input vector x\n",
    "    - preserve the height and the weight of the input images or tensors\n",
    "    - most commonly used\n",
    "\n",
    "3. Valid: $p = 0$\n",
    "    - no padding\n",
    "    - the volume of the tensors would decrease substantially\n",
    "    - may detrimental to the network performance\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the size of the convolution output\n",
    "\n",
    "The output size of a convolution is determined by the total number of times that we shift the filter **w** along the input vector\n",
    "\n",
    "Size of the input vector = n\n",
    "\n",
    "size of the filter = m\n",
    "\n",
    "padding = p\n",
    "\n",
    "stride = s\n",
    "\n",
    "The size of the output resulting from $x*w$:\n",
    "\n",
    "$$ o = \\lfloor\\frac{n+2p-m}{s}\\rfloor+1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convld Implementation:  [ 5. 14. 16. 26. 24. 34. 19. 22.]\n",
      "Numpy Results:          [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "## 1D convolutoon\n",
    "import numpy as np\n",
    "\n",
    "def convld(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    \n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape = p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "    res = []\n",
    "    \n",
    "    for i in range(0, int(len(x)/s),s):\n",
    "        res.append(np.sum(x_padded[i:i+w_rot.shape[0]]*w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "## Testing:\n",
    "x = [1,3,2,4,5,6,1,3]\n",
    "w = [1,0,3,1,2]\n",
    "\n",
    "print('Convld Implementation: ',convld(x,w,p=2,s=1))\n",
    "print('Numpy Results:         ',np.convolve(x,w,mode='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation: \n",
      " [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n",
      "Scipy Results:         \n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "## 2D convolution\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def conv2d(X, W, p=(0,0), s=(1,1)):\n",
    "    W_rot = np.array(W)[::-1,::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2 * p[0]\n",
    "    n2 = X_orig.shape[1] + 2 * p[1]\n",
    "    X_padded = np.zeros(shape=(n1,n2))\n",
    "    X_padded[p[0]:p[0]+X_orig.shape[0],\n",
    "             p[1]:p[1]+X_orig.shape[1]] = X_orig\n",
    "    res = []\n",
    "    for i in range(0,int((X_padded.shape[0] - \n",
    "                          W_rot.shape[0])/s[0])+1,s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1]-\n",
    "                               W_rot.shape[1])/s[1])+1, s[1]):\n",
    "            X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * W_rot))\n",
    "    return (np.array(res))\n",
    "\n",
    "\n",
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1 , 2,0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n",
    "print('Conv2d Implementation: \\n', \n",
    "      conv2d(X, W, p=(1,1), s=(1,1)))\n",
    "\n",
    "print('Scipy Results:         \\n', \n",
    "      scipy.signal.convolve2d(X, W, mode='same'))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-sampling\n",
    "\n",
    "Decrease the size of features, result in higher computational efficiency, also reduce the degree of overfitting.\n",
    "\n",
    "- Max-pooling: takes the maximum value from a neighborhood of pixels\n",
    "    - introduce some sort of local invariance\n",
    "    - small changes in a local neighborhood do not change the result of max-pooling\n",
    "    - generate features that are more robust to noise in the input data\n",
    "- Mean-pooling (Average-pooling): computes the average from a neighborhood of pixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (252, 221, 3)\n",
      "Number of channels: 3\n",
      "Image data type: uint8\n",
      "[[[179 134 110]\n",
      "  [182 136 112]]\n",
      "\n",
      " [[180 135 111]\n",
      "  [182 137 113]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Rex/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    }
   ],
   "source": [
    "import scipy.misc\n",
    "\n",
    "\n",
    "try:\n",
    "    img = scipy.misc.imread('./example-image.png', mode='RGB')\n",
    "except AttributeError:\n",
    "    s = (\"scipy.misc.imread requires Python's image library PIL\"\n",
    "         \" You can satisfy this requirement by installing the\"\n",
    "         \" userfriendly fork PILLOW via `pip install pillow`.\")\n",
    "    raise AttributeError(s)\n",
    "    \n",
    "    \n",
    "print('Image shape:', img.shape)\n",
    "print('Number of channels:', img.shape[2])\n",
    "print('Image data type:', img.dtype)\n",
    "\n",
    "print(img[100:102, 100:102, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing a neural network with dropout\n",
    "\n",
    "- L2\n",
    "\n",
    "- Dropout\n",
    "    - work well for regularizing (deep) neural networks\n",
    "    - the consensus (averaging) of an ensemble of models\n",
    "    - usually applied to the hidden units of higher layers\n",
    "    - during the training phase of a neural network, a fraction of the hidden units is randomly dropped at every iteration with probability $p_{drop}$ (or keep probability $p_{keep} = 1 - p_{drop}$)\n",
    "    - $p_{drop}$ is usually set to 0.5\n",
    "    - dropping a certain fraction of input neurons, the weights associated with the remaining neurons are rescaled to account for the missing (dropped) neurons\n",
    "    - forces the network to leaern a reduncdant representation of the data\n",
    "        - cannot rely on any set of hidden units since they may be turned off at any time during training and is forced to learn more general and robust patterns from the data\n",
    "        - prevent overfitting\n",
    "    - units may drop randomly during training only\n",
    "    - when evaluation, all the hidden units must be active\n",
    "        - overall activations are on the same scale during training and prediction, the activations of the active neurons have to be scaled appropriately\n",
    "    - relationship between dropout and ensemble learning:\n",
    "        - drop different hidden neurons at each iterations, effectively, we are training different models. When all these models are fully trained, we set the $p_{keep} = 1$ and use all the hidden units, meaning taking average activation from all the hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a deep convolutional neural network using TensorFlow\n",
    "\n",
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzip mnist\n",
    "\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "if (sys.version_info > (3,0)):\n",
    "    writemode = 'wb'\n",
    "else:\n",
    "    writemode = 'w'\n",
    "\n",
    "zipped_mnist = [f for f in os.listdir('./')\n",
    "                if f.endswith('ubyte.gz')]\n",
    "\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile(z, mode = 'rb') as decompressed, open(z[:-3],writemode) as outfile:\n",
    "        outfile.write(decompressed.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, Columns: 784\n",
      "Rows: 10000, Columns: 784\n",
      "Training:    (50000, 784) (50000,)\n",
      "Validation:  (10000, 784) (10000,)\n",
      "Test Set:    (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind = 'train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype = np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, n, rows, cols = struct.unpack('>IIII',\n",
    "                                 imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype = np.uint8).reshape(len(labels),784)\n",
    "    return images, labels\n",
    "\n",
    "X_data, y_data = load_mnist('./', kind = 'train')\n",
    "print('Rows: %d, Columns: %d' % (X_data.shape[0], X_data.shape[1]))\n",
    "X_test, y_test = load_mnist('./', kind = 't10k')\n",
    "print('Rows: %d, Columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "X_train, y_train = X_data[:50000,:], y_data[:50000]\n",
    "X_valid, y_valid = X_data[50000:,:], y_data[50000:]\n",
    "\n",
    "print('Training:   ', X_train.shape, y_train.shape)\n",
    "print('Validation: ', X_valid.shape, y_valid.shape)\n",
    "print('Test Set:   ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size = 64,\n",
    "                    shuffle = False, random_seed = None):\n",
    "    idx = np.arange(y.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "        \n",
    "    for i in range(0,X.shape[0], batch_size):\n",
    "        yield(X[i:i+batch_size, :], y[i:i+batch_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis = 0)\n",
    "std_val = np.std(X_train) # because some features may have zero variance, we don't compute the variance of each individual feature to avoid division-by-zero error\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_valid_centered = (X_valid - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_data, y_data, X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in TensorFlow low-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'convtest/_weights:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'convtest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"convtest/Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/net_pre-activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## wrapper functions\n",
    "\n",
    "def conv_layer(input_tensor, name,\n",
    "               kernel_size, n_output_channels,\n",
    "               padding_mode = 'SAME', strides = (1,1,1,1)):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        ## get n_input_channels:\n",
    "        ## input tensor shape:\n",
    "        ## [batch x width x height x channels_in]\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1]\n",
    "        \n",
    "        weights_shape = (list(kernel_size) + \n",
    "                         [n_input_channels, n_output_channels])\n",
    "        \n",
    "        weights = tf.get_variable(name = '_weights',\n",
    "                                  shape = weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name = '_biases',\n",
    "                                 initializer = tf.zeros(\n",
    "                                  shape = [n_output_channels]))\n",
    "        \n",
    "        print(biases)\n",
    "        conv = tf.nn.conv2d(input = input_tensor,\n",
    "                            filter = weights,\n",
    "                            strides = strides,\n",
    "                            padding = padding_mode)\n",
    "        print(conv)\n",
    "        conv = tf.nn.bias_add(conv, biases,\n",
    "                              name = 'net_pre-activation')\n",
    "        print(conv)\n",
    "        conv = tf.nn.relu(conv, name = 'activation')\n",
    "        print(conv)\n",
    "        \n",
    "        return conv\n",
    "\n",
    "## testing\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    conv_layer(x, name = 'convtest', kernel_size=(3,3), n_output_channels = 32)\n",
    "    \n",
    "del g, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fctest/_weights:0' shape=(784, 32) dtype=float32_ref>\n",
      "<tf.Variable 'fctest/biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"fctest/MatMul:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/net_pre-activation:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/activation:0\", shape=(?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def fc_layer(input_tensor, name,\n",
    "             n_output_units, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        \n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor,\n",
    "                                      shape=(-1,n_input_units))\n",
    "        weights_shape = [n_input_units, n_output_units]\n",
    "        \n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        \n",
    "        biases = tf.get_variable(name='biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                  shape=[n_output_units]))\n",
    "        print(biases)\n",
    "        \n",
    "        layer = tf.matmul(input_tensor, weights)\n",
    "        print(layer)\n",
    "        layer = tf.nn.bias_add(layer, biases,\n",
    "                               name = 'net_pre-activation')\n",
    "        print(layer)\n",
    "        \n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "        \n",
    "        layer = activation_fn(layer, name = 'activation')\n",
    "        print(layer)\n",
    "        \n",
    "        return layer\n",
    "        \n",
    "## testing\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32,\n",
    "                       shape = [None, 28, 28,1])\n",
    "    fc_layer(x, name = 'fctest', n_output_units = 32,\n",
    "             activation_fn = tf.nn.relu)\n",
    "\n",
    "del g, x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape =[None,784],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "    \n",
    "    # reshape x to a 4D tensor:\n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1,28,28,1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "    \n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    h1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    \n",
    "    ## MaxPooling\n",
    "    h1_pool = tf.nn.max_pool(h1, \n",
    "                             ksize=[1,2,2,1],\n",
    "                             strides=[1,2,2,1],\n",
    "                             padding='SAME')\n",
    "    \n",
    "    ## 2nd layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    h2 = conv_layer(h1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    \n",
    "    ## MaxPooling\n",
    "    h2_pool = tf.nn.max_pool(h2,\n",
    "                             ksize = [1,2,2,1],\n",
    "                             strides = [1,2,2,1],\n",
    "                             padding='SAME')\n",
    "    \n",
    "    ## 3rd layer: Fully Connected\n",
    "    print('\\nBuilding 3rd layer:')\n",
    "    h3 = fc_layer(h2_pool, name='fc_3',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "    \n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    h3_drop = tf.nn.dropout(h3,keep_prob=keep_prob,\n",
    "                            name='dropout_layer')\n",
    "    \n",
    "    ## 4th layer: Fully connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    h4 = fc_layer(h3_drop, name='fc_4',\n",
    "                  n_output_units=10,\n",
    "                  activation_fn = None)\n",
    "    \n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities': tf.nn.softmax(h4, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(h4, axis = 1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "    \n",
    "    \n",
    "    ## Visualize the graph with TensorBoard\n",
    "    \n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits = h4, labels = tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "    \n",
    "    ## Optimizer:\n",
    "    ## Adam: robust gradient-based optimization method\n",
    "    ## for nonconvex optimization and machine learning problems\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name = 'train_op')\n",
    "    \n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "    \n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name = 'accuracy')\n",
    "    \n",
    "def save(saver, sess, epoch, path = './model/'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in %s' % path)\n",
    "    saver.save(sess, os.path.join(path,'cnn-model.ckpt'),\n",
    "               global_step = epoch)\n",
    "    \n",
    "def load(saver, sess, path, epoch):\n",
    "    print('Loading model from %s' % path)\n",
    "    saver.restore(sess, os.path.join(\n",
    "        path,'cnn-model.ckpt-%d' % epoch))\n",
    "    \n",
    "    \n",
    "def train(sess, training_set, validation_set=None,\n",
    "          initialize = True, epochs = 20, shuffle = True,\n",
    "          dropout = 0.5, random_seed = None):\n",
    "    \n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    ## initialize variables\n",
    "    \n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    np.random.seed(random_seed) # for shuflling in batch_generator\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        batch_gen = batch_generator(X_data, y_data,\n",
    "                                    shuffle = shuffle)\n",
    "        avg_loss = 0.0\n",
    "        for i, (batch_x, batch_y) in enumerate(batch_gen):\n",
    "            feed = {'tf_x:0':batch_x,\n",
    "                    'tf_y:0':batch_y,\n",
    "                    'fc_keep_prob:0':dropout}\n",
    "            loss, _ = sess.run(\n",
    "                ['cross_entropy_loss:0', 'train_op'],\n",
    "                feed_dict = feed)\n",
    "            avg_loss += loss\n",
    "        \n",
    "        training_loss.append(avg_loss/(i+1))\n",
    "        print('Epoch %02d Training Avg. Loss: %7.3f' % (\n",
    "            epoch, avg_loss), end = ' ')\n",
    "        \n",
    "        if validation_set is not None:\n",
    "            feed = {'tf_x:0' : validation_set[0],\n",
    "                    'tf_y:0' : validation_set[1],\n",
    "                    'fc_keep_prob:0':1.0}\n",
    "            valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
    "            print('Validation Acc: %7.3f' % valid_acc)\n",
    "            \n",
    "        else:\n",
    "            print()\n",
    "\n",
    "def predict(sess, X_test, return_proba = False):\n",
    "    \n",
    "    feed = {'tf_x:0' : X_test,\n",
    "            'fc_keep_prob:0' : 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0', feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run('labels:0', feed_dict=feed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer: \n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      "Building 2nd layer: \n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "## create a graph\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn()\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Training Avg. Loss: 274.909 Validation Acc:   0.970\n",
      "Epoch 02 Training Avg. Loss:  76.410 Validation Acc:   0.981\n",
      "Epoch 03 Training Avg. Loss:  52.453 Validation Acc:   0.986\n",
      "Epoch 04 Training Avg. Loss:  40.269 Validation Acc:   0.988\n",
      "Epoch 05 Training Avg. Loss:  33.156 Validation Acc:   0.989\n",
      "Epoch 06 Training Avg. Loss:  26.888 Validation Acc:   0.989\n",
      "Epoch 07 Training Avg. Loss:  22.968 Validation Acc:   0.989\n",
      "Epoch 08 Training Avg. Loss:  19.933 Validation Acc:   0.989\n",
      "Epoch 09 Training Avg. Loss:  17.323 Validation Acc:   0.991\n",
      "Epoch 10 Training Avg. Loss:  15.149 Validation Acc:   0.991\n",
      "Epoch 11 Training Avg. Loss:  12.900 Validation Acc:   0.991\n",
      "Epoch 12 Training Avg. Loss:  11.531 Validation Acc:   0.991\n",
      "Epoch 13 Training Avg. Loss:  10.342 Validation Acc:   0.991\n",
      "Epoch 14 Training Avg. Loss:   9.057 Validation Acc:   0.990\n",
      "Epoch 15 Training Avg. Loss:   7.471 Validation Acc:   0.991\n",
      "Epoch 16 Training Avg. Loss:   7.561 Validation Acc:   0.991\n",
      "Epoch 17 Training Avg. Loss:   6.802 Validation Acc:   0.991\n",
      "Epoch 18 Training Avg. Loss:   5.878 Validation Acc:   0.992\n",
      "Epoch 19 Training Avg. Loss:   4.861 Validation Acc:   0.992\n",
      "Epoch 20 Training Avg. Loss:   4.800 Validation Acc:   0.992\n",
      "Saving model in ./model/\n"
     ]
    }
   ],
   "source": [
    "## create a TF session\n",
    "## and train the CNN model\n",
    "\n",
    "with tf.Session(graph = g) as sess:\n",
    "    train(sess,\n",
    "          training_set=(X_train_centered, y_train),\n",
    "          validation_set = (X_valid_centered, y_valid),\n",
    "          initialize = True,\n",
    "          random_seed = 123)\n",
    "    save(saver, sess, epoch = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1ac876243a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### restoring the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m## create a new graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "### Calculate prediction accurarcy\n",
    "### on test set\n",
    "### restoring the saved model\n",
    "\n",
    "del g\n",
    "\n",
    "## create a new graph\n",
    "## and build the model\n",
    "\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn()\n",
    "    \n",
    "    ## saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "## create a new session\n",
    "## and restore the model\n",
    "\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    load(saver, sess,\n",
    "         epoch = 20, path = './model/')\n",
    "    preds = predict(sess, X_test_centered, return_proba = False)\n",
    "    print('Test Accuracy: %.3f%%' % (100*np.sum(preds == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-20\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "## run the predicition on\n",
    "##  some test samples\n",
    "\n",
    "np.set_printoptions(precision=2,suppress=True)\n",
    "\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess,\n",
    "         epoch = 20, path = './model/')\n",
    "    print(predict(sess, X_test_centered[:10],\n",
    "                  return_proba = False))\n",
    "    print(predict(sess, X_test_centered[:10],\n",
    "                  return_proba = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-20\n",
      "Epoch 01 Training Avg. Loss:   4.122 Validation Acc:   0.992\n",
      "Epoch 02 Training Avg. Loss:   4.114 Validation Acc:   0.991\n",
      "Epoch 03 Training Avg. Loss:   4.175 Validation Acc:   0.992\n",
      "Epoch 04 Training Avg. Loss:   2.934 Validation Acc:   0.992\n",
      "Epoch 05 Training Avg. Loss:   3.770 Validation Acc:   0.991\n",
      "Epoch 06 Training Avg. Loss:   2.635 Validation Acc:   0.992\n",
      "Epoch 07 Training Avg. Loss:   2.736 Validation Acc:   0.992\n",
      "Epoch 08 Training Avg. Loss:   2.814 Validation Acc:   0.991\n",
      "Epoch 09 Training Avg. Loss:   2.783 Validation Acc:   0.992\n",
      "Epoch 10 Training Avg. Loss:   2.044 Validation Acc:   0.992\n",
      "Epoch 11 Training Avg. Loss:   2.090 Validation Acc:   0.993\n",
      "Epoch 12 Training Avg. Loss:   2.752 Validation Acc:   0.992\n",
      "Epoch 13 Training Avg. Loss:   1.810 Validation Acc:   0.992\n",
      "Epoch 14 Training Avg. Loss:   1.444 Validation Acc:   0.993\n",
      "Epoch 15 Training Avg. Loss:   2.081 Validation Acc:   0.992\n",
      "Epoch 16 Training Avg. Loss:   1.813 Validation Acc:   0.992\n",
      "Epoch 17 Training Avg. Loss:   1.618 Validation Acc:   0.993\n",
      "Epoch 18 Training Avg. Loss:   1.360 Validation Acc:   0.992\n",
      "Epoch 19 Training Avg. Loss:   1.743 Validation Acc:   0.993\n",
      "Epoch 20 Training Avg. Loss:   1.224 Validation Acc:   0.991\n",
      "Saving model in ./model/\n",
      "Test Accuracy: 99.210%\n"
     ]
    }
   ],
   "source": [
    "## continue training for 20 more epochs\n",
    "## without re-initializing :: initialize = False\n",
    "## create a new session\n",
    "## and restore the model\n",
    "\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    load(saver, sess, epoch = 20, path = './model/')\n",
    "    train(sess,\n",
    "          training_set = (X_train_centered, y_train),\n",
    "          validation_set = (X_valid_centered, y_valid),\n",
    "          initialize = False, # reuse the parameters\n",
    "          epochs = 20,\n",
    "          random_seed = 123)\n",
    "    save(saver, sess, epoch = 40, path = './model/')\n",
    "    preds = predict(sess, X_test_centered,\n",
    "                    return_proba = False)\n",
    "    print('Test Accuracy: %.3f%%' % (100*np.sum(preds==y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in the TensorFlow Layers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ConvNN(object):\n",
    "    def __init__(self, batchsize = 64,\n",
    "                 epochs = 20, learning_rate = 1e-4,\n",
    "                 dropout_rate = 0.5,\n",
    "                 shuffle = True, random_seed = None):\n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate =learning_rate\n",
    "        self.dropout_rate =dropout_rate\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            ## set random-seed\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            ## build the network\n",
    "            self.build()\n",
    "            \n",
    "            ## initializer\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "            ## saver\n",
    "            self.saver = tf.train.Saver()\n",
    "        \n",
    "        ## create a session\n",
    "        self.sess = tf.Session(graph = g)\n",
    "        \n",
    "    def build(self):\n",
    "        ## Placeholders for X and y\n",
    "        tf_x = tf.placeholder(tf.float32,\n",
    "                              shape = [None, 784],\n",
    "                              name = 'tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32,\n",
    "                              shape = [None],\n",
    "                              name = 'tf_y')\n",
    "        is_train = tf.placeholder(tf.bool,\n",
    "                                  shape = (),\n",
    "                                  name = 'is_train')\n",
    "        \n",
    "        ## reshape x to a 4D tensor:\n",
    "        tf_x_image = tf.reshape(tf_x, shape = [-1,28,28,1],\n",
    "                                name = 'input_x_2dimages')\n",
    "        ## One-hot encoding\n",
    "        tf_y_onehot = tf.one_hot(indices = tf_y, depth = 10,\n",
    "                                 dtype = tf.float32,\n",
    "                                 name = 'input_y_onehot')\n",
    "        ## 1st layer: Conv_1\n",
    "        h1 = tf.layers.conv2d(tf_x_image,\n",
    "                              kernel_size = (5,5),\n",
    "                              filters = 32,\n",
    "                              activation = tf.nn.relu)\n",
    "        ## MaxPooling\n",
    "        h1_pool =tf.layers.max_pooling2d(h1,\n",
    "                                         pool_size = (2,2),\n",
    "                                         strides = (2,2))\n",
    "        ## 2nd layer: Conv_2\n",
    "        h2 = tf.layers.conv2d(h1_pool, kernel_size = (5,5),\n",
    "                              filters = 64,\n",
    "                              activation = tf.nn.relu)\n",
    "        ## MaxPooling\n",
    "        h2_pool = tf.layers.max_pooling2d(h2,\n",
    "                                          pool_size = (2,2),\n",
    "                                          strides = (2,2))\n",
    "        ## 3rd layer: Fully Connected\n",
    "        input_shape = h2_pool.get_shape().as_list()\n",
    "        n_input_units = np.prod(input_shape[1:])\n",
    "        h2_pool_flat = tf.reshape(h2_pool,\n",
    "                                  shape=[-1,n_input_units])\n",
    "        h3 = tf.layers.dense(h2_pool_flat,1024,\n",
    "                             activation = tf.nn.relu)\n",
    "        \n",
    "        ## Dropout\n",
    "        h3_drop = tf.layers.dropout(h3,\n",
    "                                    rate = self.dropout_rate,\n",
    "                                    training= is_train)\n",
    "        \n",
    "        ## 4th layer: Fully Connected (Linear activation)\n",
    "        h4 = tf.layers.dense(h3_drop, 10, activation = None)\n",
    "        \n",
    "        ## Prediction\n",
    "        predictions = {\n",
    "            'probabilities': tf.nn.softmax(h4,\n",
    "                                           name = 'probabilities'),\n",
    "            'labels': tf.cast(tf.argmax(h4, axis = 1),\n",
    "                              tf.int32, name = 'labels')\n",
    "        }\n",
    "        \n",
    "        ## Loss Function and Optimization\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits = h4, labels = tf_y_onehot),\n",
    "            name = 'cross_entropy_loss')\n",
    "        \n",
    "        ## Optimizer:\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                       name = 'train_op')\n",
    "        \n",
    "        ## Finding accuracy\n",
    "        correct_predictions = tf.equal(\n",
    "            predictions['labels'],\n",
    "            tf_y, name = 'correct_preds')\n",
    "        \n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name = 'accuracy')\n",
    "        \n",
    "    def save(self,epoch, path = './tflayers-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        print('Saving model in %s' % path)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(path, 'model.ckpt'),\n",
    "                        global_step = epoch)\n",
    "\n",
    "    def load(self, epoch, path):\n",
    "        print('Loading model from %s' % path)\n",
    "        self.saver.restore(self.sess,\n",
    "                               os.path.join(path, 'model.ckpt-%d' % epoch))\n",
    "    def train(self, training_set, \n",
    "              validation_set=None,\n",
    "              initialize=True):\n",
    "        ## initialize variables\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "\n",
    "        self.train_cost_ = []\n",
    "        X_data = np.array(training_set[0])\n",
    "        y_data = np.array(training_set[1])\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            batch_gen = \\\n",
    "                batch_generator(X_data, y_data, \n",
    "                                 shuffle=self.shuffle)\n",
    "            avg_loss = 0.0\n",
    "            for i, (batch_x,batch_y) in \\\n",
    "                enumerate(batch_gen):\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': True} ## for dropout\n",
    "                loss, _ = self.sess.run(\n",
    "                        ['cross_entropy_loss:0', 'train_op'], \n",
    "                        feed_dict=feed)\n",
    "                avg_loss += loss\n",
    "                \n",
    "            print('Epoch %02d: Training Avg. Loss: '\n",
    "                  '%7.3f' % (epoch, avg_loss), end=' ')\n",
    "            if validation_set is not None:\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': False} ## for dropout\n",
    "                valid_acc = self.sess.run('accuracy:0',\n",
    "                                          feed_dict=feed)\n",
    "                print('Validation Acc: %7.3f' % valid_acc)\n",
    "            else:\n",
    "                print()\n",
    "                    \n",
    "    def predict(self,X_test, return_proba = False):\n",
    "        feed = {'tf_x:0': X_test,\n",
    "                'is_train:0': False} ## for dropout\n",
    "        if return_proba:\n",
    "            return self.sess.run('probabilities:0',\n",
    "                                 feed_dict = feed)\n",
    "        else:\n",
    "            return self.sess.run('labels:0',\n",
    "                                 feed_dict = feed)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ConvNN(random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Training Avg. Loss: 268.988 Validation Acc:   1.000\n",
      "Epoch 02: Training Avg. Loss:  72.877 Validation Acc:   1.000\n",
      "Epoch 03: Training Avg. Loss:  50.849 Validation Acc:   1.000\n",
      "Epoch 04: Training Avg. Loss:  39.729 Validation Acc:   1.000\n",
      "Epoch 05: Training Avg. Loss:  31.645 Validation Acc:   1.000\n",
      "Epoch 06: Training Avg. Loss:  27.273 Validation Acc:   1.000\n",
      "Epoch 07: Training Avg. Loss:  23.623 Validation Acc:   1.000\n",
      "Epoch 08: Training Avg. Loss:  19.688 Validation Acc:   1.000\n",
      "Epoch 09: Training Avg. Loss:  17.174 Validation Acc:   1.000\n",
      "Epoch 10: Training Avg. Loss:  15.224 Validation Acc:   1.000\n",
      "Epoch 11: Training Avg. Loss:  12.601 Validation Acc:   1.000\n",
      "Epoch 12: Training Avg. Loss:  11.411 Validation Acc:   1.000\n",
      "Epoch 13: Training Avg. Loss:  10.055 Validation Acc:   1.000\n",
      "Epoch 14: Training Avg. Loss:   9.747 Validation Acc:   1.000\n",
      "Epoch 15: Training Avg. Loss:   8.242 Validation Acc:   1.000\n",
      "Epoch 16: Training Avg. Loss:   7.838 Validation Acc:   1.000\n",
      "Epoch 17: Training Avg. Loss:   6.629 Validation Acc:   1.000\n",
      "Epoch 18: Training Avg. Loss:   6.258 Validation Acc:   1.000\n",
      "Epoch 19: Training Avg. Loss:   5.349 Validation Acc:   1.000\n",
      "Epoch 20: Training Avg. Loss:   4.657 Validation Acc:   1.000\n",
      "Saving model in ./tflayers-model/\n"
     ]
    }
   ],
   "source": [
    "cnn.train(training_set = (X_train_centered, y_train),\n",
    "          validation_set = (X_valid_centered, y_valid))\n",
    "cnn.save(epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./tflayers-model/\n",
      "INFO:tensorflow:Restoring parameters from ./tflayers-model/model.ckpt-20\n",
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "del cnn\n",
    "\n",
    "cnn2 = ConvNN(random_seed=123)\n",
    "cnn2.load(epoch=20, path = './tflayers-model/')\n",
    "print(cnn2.predict(X_test_centered[:10,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.33%\n"
     ]
    }
   ],
   "source": [
    "preds = cnn2.predict(X_test_centered)\n",
    "print('Test Accuracy: %.2f%%' % (100*np.sum(y_test==preds)/len(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
